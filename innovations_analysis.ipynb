{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "from plotly.subplots import make_subplots\n",
    "import pyalex\n",
    "from pyalex import Works\n",
    "\n",
    "from research_impact import plotting\n",
    "from research_impact.plotting import save_plot\n",
    "from research_impact.utils import dict_to_dataarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The polite pool has much faster and more consistent response times. To get into the polite pool, you set your email:\n",
    "pyalex.config.email = \"ben@epochai.org\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.templates.default = \"plotly_white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_location = 'data/innovations/'\n",
    "os.makedirs(data_file_location, exist_ok=True)\n",
    "snapshot_datestring = '2023-11-06'\n",
    "\n",
    "result_file_location = 'results/innovations/'\n",
    "os.makedirs(result_file_location, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "# Snapshot\n",
    "origins_df = pd.read_csv(data_file_location + f\"innovation_origins_snapshot_{snapshot_datestring}.csv\")\n",
    "\n",
    "# Live data\n",
    "# sheet_id = '1L_j7OaX19HXWWIx_apKvWo2OteY1XOB7FamaLEd_p0s'\n",
    "# tab_id = '578731623'\n",
    "# data_url = f'https://docs.google.com/spreadsheets/d/{sheet_id}/export?gid={tab_id}&format=csv'\n",
    "# origins_df = pd.read_csv(data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithmic innovation</th>\n",
       "      <th>Origin title</th>\n",
       "      <th>Origin link</th>\n",
       "      <th>Origin publication date</th>\n",
       "      <th>Origin affiliations</th>\n",
       "      <th>Origin authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kaplan et al. scaling laws</td>\n",
       "      <td>Scaling Laws for Neural Language Models</td>\n",
       "      <td>https://arxiv.org/abs/2001.08361</td>\n",
       "      <td>2020-Jan-23</td>\n",
       "      <td>Johns Hopkins University; OpenAI</td>\n",
       "      <td>Jared Kaplan, Sam McCandlish, Tom Henighan, To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hoffmann et al. scaling laws</td>\n",
       "      <td>Training Compute-Optimal Large Language Models</td>\n",
       "      <td>https://arxiv.org/abs/2203.15556</td>\n",
       "      <td>2022-Mar-29</td>\n",
       "      <td>Google DeepMind</td>\n",
       "      <td>Jordan Hoffmann, Sebastian Borgeaud, Arthur Me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformer (general)</td>\n",
       "      <td>Attention Is All You Need</td>\n",
       "      <td>https://arxiv.org/abs/1706.03762</td>\n",
       "      <td>2017-Jun-12</td>\n",
       "      <td>Google Brain; Google Research; University of T...</td>\n",
       "      <td>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sparse Attention</td>\n",
       "      <td>Generating Long Sequences with Sparse Transfor...</td>\n",
       "      <td>https://arxiv.org/abs/1904.10509</td>\n",
       "      <td>2019-Apr-23</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>Rewon Child, Scott Gray, Alec Radford, Ilya Su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linear Attention</td>\n",
       "      <td>Transformers are RNNs: Fast Autoregressive Tra...</td>\n",
       "      <td>http://proceedings.mlr.press/v119/katharopoulo...</td>\n",
       "      <td>2020-Jan-01</td>\n",
       "      <td>Idiap Research Institute, Switzerland; EPFL, S...</td>\n",
       "      <td>Angelos Katharopoulos, Apoorv Vyas, Nikolaos P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Algorithmic innovation  \\\n",
       "0    Kaplan et al. scaling laws   \n",
       "1  Hoffmann et al. scaling laws   \n",
       "2         Transformer (general)   \n",
       "3              Sparse Attention   \n",
       "4              Linear Attention   \n",
       "\n",
       "                                        Origin title  \\\n",
       "0            Scaling Laws for Neural Language Models   \n",
       "1     Training Compute-Optimal Large Language Models   \n",
       "2                          Attention Is All You Need   \n",
       "3  Generating Long Sequences with Sparse Transfor...   \n",
       "4  Transformers are RNNs: Fast Autoregressive Tra...   \n",
       "\n",
       "                                         Origin link Origin publication date  \\\n",
       "0                   https://arxiv.org/abs/2001.08361             2020-Jan-23   \n",
       "1                   https://arxiv.org/abs/2203.15556             2022-Mar-29   \n",
       "2                   https://arxiv.org/abs/1706.03762             2017-Jun-12   \n",
       "3                   https://arxiv.org/abs/1904.10509             2019-Apr-23   \n",
       "4  http://proceedings.mlr.press/v119/katharopoulo...             2020-Jan-01   \n",
       "\n",
       "                                 Origin affiliations  \\\n",
       "0                   Johns Hopkins University; OpenAI   \n",
       "1                                    Google DeepMind   \n",
       "2  Google Brain; Google Research; University of T...   \n",
       "3                                             OpenAI   \n",
       "4  Idiap Research Institute, Switzerland; EPFL, S...   \n",
       "\n",
       "                                      Origin authors  \n",
       "0  Jared Kaplan, Sam McCandlish, Tom Henighan, To...  \n",
       "1  Jordan Hoffmann, Sebastian Borgeaud, Arthur Me...  \n",
       "2  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jak...  \n",
       "3  Rewon Child, Scott Gray, Alec Radford, Ilya Su...  \n",
       "4  Angelos Katharopoulos, Apoorv Vyas, Nikolaos P...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origins_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithmic innovation</th>\n",
       "      <th>Origin title</th>\n",
       "      <th>Origin link</th>\n",
       "      <th>Origin publication date</th>\n",
       "      <th>Origin affiliations</th>\n",
       "      <th>Origin authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kaplan et al. scaling laws</td>\n",
       "      <td>Scaling Laws for Neural Language Models</td>\n",
       "      <td>https://arxiv.org/abs/2001.08361</td>\n",
       "      <td>2020-Jan-23</td>\n",
       "      <td>Johns Hopkins University; OpenAI</td>\n",
       "      <td>Jared Kaplan, Sam McCandlish, Tom Henighan, To...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hoffmann et al. scaling laws</td>\n",
       "      <td>Training Compute-Optimal Large Language Models</td>\n",
       "      <td>https://arxiv.org/abs/2203.15556</td>\n",
       "      <td>2022-Mar-29</td>\n",
       "      <td>Google DeepMind</td>\n",
       "      <td>Jordan Hoffmann, Sebastian Borgeaud, Arthur Me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Transformer (general)</td>\n",
       "      <td>Attention Is All You Need</td>\n",
       "      <td>https://arxiv.org/abs/1706.03762</td>\n",
       "      <td>2017-Jun-12</td>\n",
       "      <td>Google Brain; Google Research; University of T...</td>\n",
       "      <td>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sparse Attention</td>\n",
       "      <td>Generating Long Sequences with Sparse Transfor...</td>\n",
       "      <td>https://arxiv.org/abs/1904.10509</td>\n",
       "      <td>2019-Apr-23</td>\n",
       "      <td>OpenAI</td>\n",
       "      <td>Rewon Child, Scott Gray, Alec Radford, Ilya Su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Linear Attention</td>\n",
       "      <td>Transformers are RNNs: Fast Autoregressive Tra...</td>\n",
       "      <td>http://proceedings.mlr.press/v119/katharopoulo...</td>\n",
       "      <td>2020-Jan-01</td>\n",
       "      <td>Idiap Research Institute, Switzerland; EPFL, S...</td>\n",
       "      <td>Angelos Katharopoulos, Apoorv Vyas, Nikolaos P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Algorithmic innovation  \\\n",
       "0    Kaplan et al. scaling laws   \n",
       "1  Hoffmann et al. scaling laws   \n",
       "2         Transformer (general)   \n",
       "3              Sparse Attention   \n",
       "4              Linear Attention   \n",
       "\n",
       "                                        Origin title  \\\n",
       "0            Scaling Laws for Neural Language Models   \n",
       "1     Training Compute-Optimal Large Language Models   \n",
       "2                          Attention Is All You Need   \n",
       "3  Generating Long Sequences with Sparse Transfor...   \n",
       "4  Transformers are RNNs: Fast Autoregressive Tra...   \n",
       "\n",
       "                                         Origin link Origin publication date  \\\n",
       "0                   https://arxiv.org/abs/2001.08361             2020-Jan-23   \n",
       "1                   https://arxiv.org/abs/2203.15556             2022-Mar-29   \n",
       "2                   https://arxiv.org/abs/1706.03762             2017-Jun-12   \n",
       "3                   https://arxiv.org/abs/1904.10509             2019-Apr-23   \n",
       "4  http://proceedings.mlr.press/v119/katharopoulo...             2020-Jan-01   \n",
       "\n",
       "                                 Origin affiliations  \\\n",
       "0                   Johns Hopkins University; OpenAI   \n",
       "1                                    Google DeepMind   \n",
       "2  Google Brain; Google Research; University of T...   \n",
       "3                                             OpenAI   \n",
       "4  Idiap Research Institute, Switzerland; EPFL, S...   \n",
       "\n",
       "                                      Origin authors  \n",
       "0  Jared Kaplan, Sam McCandlish, Tom Henighan, To...  \n",
       "1  Jordan Hoffmann, Sebastian Borgeaud, Arthur Me...  \n",
       "2  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jak...  \n",
       "3  Rewon Child, Scott Gray, Alec Radford, Ilya Su...  \n",
       "4  Angelos Katharopoulos, Apoorv Vyas, Nikolaos P...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origins_df.dropna(subset=['Origin affiliations'], inplace=True)\n",
    "origins_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create institution => origins mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "institution_aliases = {\n",
    "    'Google': 'Google',\n",
    "    'Google Brain': 'Google',\n",
    "    'Google Research': 'Google',\n",
    "    'DeepMind': 'DeepMind',\n",
    "    'Google DeepMind': 'DeepMind',\n",
    "    'OpenAI': 'OpenAI',\n",
    "    'Baidu Research': 'Baidu',\n",
    "    'NVIDIA': 'NVIDIA',\n",
    "    'Facebook AI Research': 'Meta',\n",
    "    'Facebook AI': 'Meta',\n",
    "    'Zhuiyi Technology Co., Ltd.': 'Zhuiyi',\n",
    "    'Microsoft Research': 'Microsoft',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Johns Hopkins University - no alias\n",
      "University of Toronto - no alias\n",
      "Idiap Research Institute, Switzerland - no alias\n",
      "EPFL, Switzerland - no alias\n",
      "University of Washington, Seattle - no alias\n",
      "University of Geneva - no alias\n",
      "UC Berkeley - no alias\n",
      "University of Edinburgh - no alias\n",
      "University of Zurich - no alias\n",
      "University of Toronto - no alias\n",
      "University of Toronto - no alias\n",
      "Carnegie Mellon University - no alias\n",
      "University of Washington - no alias\n",
      "Allen Institute for AI - no alias\n",
      "University of Toronto - no alias\n",
      "University of Chicago - no alias\n",
      "Toyota Technological Institute at Chicago - no alias\n",
      "Jagiellonian University - no alias\n",
      "University of Toronto - no alias\n",
      "University of Freiburg - no alias\n",
      "University of Amsterdam - no alias\n",
      "University of Toronto - no alias\n",
      "University of Freiburg - no alias\n",
      "UC Berkeley - no alias\n",
      "University of Toronto - no alias\n",
      "University of Montreal - no alias\n",
      "Brno University - no alias\n",
      "Stanford University - no alias\n",
      "University at Buffalo, SUNY - no alias\n",
      "University of Montreal - no alias\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'OpenAI': ['Kaplan et al. scaling laws',\n",
       "              'Sparse Attention',\n",
       "              'Instruction tuning',\n",
       "              'RLHF',\n",
       "              'PPO',\n",
       "              'Prompting for in-context learning'],\n",
       "             'DeepMind': ['Hoffmann et al. scaling laws', 'RLHF', 'A2C'],\n",
       "             'Google': ['Transformer (general)',\n",
       "              'Transformer (general)',\n",
       "              'Attention with locality-sensitive hashing',\n",
       "              'Multi-Query Attention',\n",
       "              'Grouped Query Attention',\n",
       "              'LayerNorm',\n",
       "              'Sinusoidal position embeddings',\n",
       "              'Sinusoidal position embeddings',\n",
       "              'Relative position embeddings',\n",
       "              'SwiGLU activation',\n",
       "              'Sparsely-Gated Mixture-of-Experts layer (MoE)',\n",
       "              'Encoder-decoder Transformer',\n",
       "              'Encoder-decoder Transformer',\n",
       "              'Causal decoder Transformer (decoder-only)',\n",
       "              'Language modeling task (with Transformer architecture)',\n",
       "              'Cloze task (with Transformer architecture)',\n",
       "              'Mixture-of-Denoisers task',\n",
       "              'Dynamic batch size',\n",
       "              'Adafactor optimizer',\n",
       "              'Chain-of-thought'],\n",
       "             'Meta': ['Pre-normalization',\n",
       "              'Learnable position embeddings',\n",
       "              'ALiBi positional encodings',\n",
       "              'Denoising autoencoding task (with Transformer architecture)'],\n",
       "             'Zhuiyi': ['Rotary position embeddings'],\n",
       "             'Microsoft': ['Prefix decoder Transformer (decoder-only)'],\n",
       "             'Baidu': ['Mixed precision training'],\n",
       "             'NVIDIA': ['Mixed precision training']})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "institution_key_algorithms = defaultdict(list)\n",
    "for i, row in origins_df.iterrows():\n",
    "    algorithm_name = row['Algorithmic innovation']\n",
    "    affiliations = row['Origin affiliations']\n",
    "    affiliations = [affiliation.strip() for affiliation in affiliations.split(';')]\n",
    "    for affiliation in affiliations:\n",
    "        if institution_aliases.get(affiliation) is not None:\n",
    "            alias = institution_aliases[affiliation]\n",
    "            institution_key_algorithms[alias].append(algorithm_name)\n",
    "        else:\n",
    "            print(affiliation, '- no alias')\n",
    "institution_key_algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.DataArray (institution: 8)&gt;\n",
       "array([ 1.,  3., 20.,  4.,  1.,  1.,  6.,  1.])\n",
       "Coordinates:\n",
       "  * institution  (institution) &lt;U9 &#x27;Baidu&#x27; &#x27;DeepMind&#x27; ... &#x27;OpenAI&#x27; &#x27;Zhuiyi&#x27;</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'></div><ul class='xr-dim-list'><li><span class='xr-has-index'>institution</span>: 8</li></ul></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-63b93d82-c6a1-43da-8c16-030c37aa2142' class='xr-array-in' type='checkbox' checked><label for='section-63b93d82-c6a1-43da-8c16-030c37aa2142' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>1.0 3.0 20.0 4.0 1.0 1.0 6.0 1.0</span></div><div class='xr-array-data'><pre>array([ 1.,  3., 20.,  4.,  1.,  1.,  6.,  1.])</pre></div></div></li><li class='xr-section-item'><input id='section-31508590-089b-41ec-af8e-ae98ebf681e1' class='xr-section-summary-in' type='checkbox'  checked><label for='section-31508590-089b-41ec-af8e-ae98ebf681e1' class='xr-section-summary' >Coordinates: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>institution</span></div><div class='xr-var-dims'>(institution)</div><div class='xr-var-dtype'>&lt;U9</div><div class='xr-var-preview xr-preview'>&#x27;Baidu&#x27; &#x27;DeepMind&#x27; ... &#x27;Zhuiyi&#x27;</div><input id='attrs-1a55860b-9eaf-49f9-a632-698c53a4511e' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-1a55860b-9eaf-49f9-a632-698c53a4511e' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-ab43e864-d88d-4800-ad46-cfe0a23369ec' class='xr-var-data-in' type='checkbox'><label for='data-ab43e864-d88d-4800-ad46-cfe0a23369ec' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([&#x27;Baidu&#x27;, &#x27;DeepMind&#x27;, &#x27;Google&#x27;, &#x27;Meta&#x27;, &#x27;Microsoft&#x27;, &#x27;NVIDIA&#x27;, &#x27;OpenAI&#x27;,\n",
       "       &#x27;Zhuiyi&#x27;], dtype=&#x27;&lt;U9&#x27;)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-d84b34d8-a2b3-41cf-bbf7-c58e4b1e55ba' class='xr-section-summary-in' type='checkbox'  ><label for='section-d84b34d8-a2b3-41cf-bbf7-c58e4b1e55ba' class='xr-section-summary' >Indexes: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>institution</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-b9bbdd67-f8b8-4261-a23c-d68770db9a5e' class='xr-index-data-in' type='checkbox'/><label for='index-b9bbdd67-f8b8-4261-a23c-d68770db9a5e' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Index([&#x27;Baidu&#x27;, &#x27;DeepMind&#x27;, &#x27;Google&#x27;, &#x27;Meta&#x27;, &#x27;Microsoft&#x27;, &#x27;NVIDIA&#x27;, &#x27;OpenAI&#x27;,\n",
       "       &#x27;Zhuiyi&#x27;],\n",
       "      dtype=&#x27;object&#x27;, name=&#x27;institution&#x27;))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-edddcff9-7027-4744-847f-c474d57ba600' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-edddcff9-7027-4744-847f-c474d57ba600' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.DataArray (institution: 8)>\n",
       "array([ 1.,  3., 20.,  4.,  1.,  1.,  6.,  1.])\n",
       "Coordinates:\n",
       "  * institution  (institution) <U9 'Baidu' 'DeepMind' ... 'OpenAI' 'Zhuiyi'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "institution_key_algorithms_count = dict_to_dataarray(institution_key_algorithms, dim='institution', val_fn=len)\n",
    "institution_key_algorithms_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Key innovations",
         "type": "bar",
         "x": [
          "Baidu",
          "DeepMind",
          "Google",
          "Meta",
          "Microsoft",
          "NVIDIA",
          "OpenAI",
          "Zhuiyi"
         ],
         "y": [
          1,
          3,
          20,
          4,
          1,
          1,
          6,
          1
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 12
          },
          "showarrow": false,
          "text": "Number of innovations for LLMs",
          "x": -0.06,
          "xref": "x domain",
          "y": 1.15,
          "yref": "y domain"
         }
        ],
        "autosize": false,
        "font": {
         "size": 10
        },
        "height": 300,
        "legend": {
         "x": 0.99,
         "xanchor": "right",
         "y": 0.99,
         "yanchor": "top"
        },
        "margin": {
         "b": 20,
         "l": 20,
         "r": 20,
         "t": 40
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "color": "#034752",
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "marker": {
             "color": "#034752"
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "bargap": 0.3,
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f",
           "family": "Messina Sans"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "font": {
            "color": "#09323A",
            "family": "Messina Serif",
            "size": 16
           },
           "x": 0
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#5C737B",
           "linewidth": 1,
           "tickcolor": "#5C737B",
           "tickfont": {
            "color": "#435359",
            "size": 10
           },
           "ticks": "",
           "title": {
            "font": {
             "size": 12
            },
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#5C737B",
           "linewidth": 1,
           "tickcolor": "#5C737B",
           "tickfont": {
            "color": "#435359",
            "size": 10
           },
           "ticks": "",
           "title": {
            "font": {
             "size": 12
            },
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "x": 0.5
        },
        "width": 400,
        "xaxis": {
         "categoryorder": "total descending",
         "showline": true,
         "ticklen": 6,
         "ticks": "outside"
        },
        "yaxis": {
         "title": {
          "standoff": 0,
          "text": ""
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        name='Key innovations',\n",
    "        x=institution_key_algorithms_count.institution,\n",
    "        y=institution_key_algorithms_count\n",
    "    ),\n",
    "])\n",
    "\n",
    "## Plot layout\n",
    "fig.update_layout(xaxis={'categoryorder':'total descending'})\n",
    "fig.update_layout(\n",
    "    # title='Initial ranking of companies leading in AI research',\n",
    "    # xaxis_title='Company',\n",
    "    yaxis_title='Number of innovations for LLMs',\n",
    ")\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"right\",\n",
    "        x=0.99,\n",
    "    ),\n",
    ")\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=400,\n",
    "    height=300,\n",
    "    title_x=0.5,\n",
    "    font=dict(size=10),\n",
    "    margin=dict(l=20, r=20, t=20, b=20),\n",
    ")\n",
    "\n",
    "plotting.prettify_bar_chart(fig, rotate_x_labels=False)\n",
    "fig.update_layout(margin=dict(t=40))\n",
    "\n",
    "## Save plot\n",
    "save_plot(fig, result_file_location, 'num_key_innovations')\n",
    "\n",
    "## Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count occurrence of innovations directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snapshot\n",
    "occurrences_df = pd.read_csv(data_file_location + f\"innovation_occurrences_snapshot_{snapshot_datestring}.csv\", index_col='Algorithmic innovation')\n",
    "\n",
    "# Live data\n",
    "# sheet_id = '1L_j7OaX19HXWWIx_apKvWo2OteY1XOB7FamaLEd_p0s'\n",
    "# tab_id = '1765093800'\n",
    "# data_url = f'https://docs.google.com/spreadsheets/d/{sheet_id}/export?gid={tab_id}&format=csv'\n",
    "# occurrences_df = pd.read_csv(data_url, index_col='Algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GPT-4</th>\n",
       "      <th>PaLM 2</th>\n",
       "      <th>GPT-3.5</th>\n",
       "      <th>PaLM (540B)</th>\n",
       "      <th>Megatron-Turing NLG (530B)</th>\n",
       "      <th>ERNIE 3.0 Titan</th>\n",
       "      <th>LLaMA 2 (70B)</th>\n",
       "      <th>Gopher (280B)</th>\n",
       "      <th>Chinchilla (70B)</th>\n",
       "      <th>PanGu-Σ</th>\n",
       "      <th>...</th>\n",
       "      <th>Falcon-40B</th>\n",
       "      <th>YaLM</th>\n",
       "      <th>ALIGN</th>\n",
       "      <th>AlexaTM 20B</th>\n",
       "      <th>BLOOM (176B)</th>\n",
       "      <th>NLLB</th>\n",
       "      <th>Megatron-LM</th>\n",
       "      <th>GPT-2</th>\n",
       "      <th>GPT</th>\n",
       "      <th>Transformer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algorithmic innovation</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Link</th>\n",
       "      <td>https://arxiv.org/abs/2303.08774</td>\n",
       "      <td>https://ai.google/static/documents/palm2techre...</td>\n",
       "      <td>https://platform.openai.com/docs/models/gpt-3-5</td>\n",
       "      <td>https://arxiv.org/abs/2204.02311</td>\n",
       "      <td>https://arxiv.org/abs/2201.11990</td>\n",
       "      <td>https://arxiv.org/abs/2112.12731</td>\n",
       "      <td>https://ai.meta.com/research/publications/llam...</td>\n",
       "      <td>https://arxiv.org/abs/2112.11446</td>\n",
       "      <td>https://arxiv.org/abs/2203.15556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2208.01448</td>\n",
       "      <td>https://huggingface.co/bigscience/bloom</td>\n",
       "      <td>https://research.facebook.com/publications/no-...</td>\n",
       "      <td>https://arxiv.org/abs/1909.08053</td>\n",
       "      <td>https://cdn.openai.com/better-language-models/...</td>\n",
       "      <td>https://cdn.openai.com/research-covers/languag...</td>\n",
       "      <td>https://proceedings.neurips.cc/paper_files/pap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Included</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kaplan et al. scaling laws</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hoffmann et al. scaling laws</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transformer (general)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         GPT-4  \\\n",
       "Algorithmic innovation                                           \n",
       "Link                          https://arxiv.org/abs/2303.08774   \n",
       "Included                                                     0   \n",
       "Kaplan et al. scaling laws                                 NaN   \n",
       "Hoffmann et al. scaling laws                               NaN   \n",
       "Transformer (general)                                      NaN   \n",
       "\n",
       "                                                                         PaLM 2  \\\n",
       "Algorithmic innovation                                                            \n",
       "Link                          https://ai.google/static/documents/palm2techre...   \n",
       "Included                                                                      0   \n",
       "Kaplan et al. scaling laws                                                    0   \n",
       "Hoffmann et al. scaling laws                                                  1   \n",
       "Transformer (general)                                                         1   \n",
       "\n",
       "                                                                      GPT-3.5  \\\n",
       "Algorithmic innovation                                                          \n",
       "Link                          https://platform.openai.com/docs/models/gpt-3-5   \n",
       "Included                                                                    0   \n",
       "Kaplan et al. scaling laws                                                NaN   \n",
       "Hoffmann et al. scaling laws                                              NaN   \n",
       "Transformer (general)                                                     NaN   \n",
       "\n",
       "                                                   PaLM (540B)  \\\n",
       "Algorithmic innovation                                           \n",
       "Link                          https://arxiv.org/abs/2204.02311   \n",
       "Included                                                     1   \n",
       "Kaplan et al. scaling laws                                   1   \n",
       "Hoffmann et al. scaling laws                                 0   \n",
       "Transformer (general)                                        1   \n",
       "\n",
       "                                    Megatron-Turing NLG (530B)  \\\n",
       "Algorithmic innovation                                           \n",
       "Link                          https://arxiv.org/abs/2201.11990   \n",
       "Included                                                     1   \n",
       "Kaplan et al. scaling laws                                   1   \n",
       "Hoffmann et al. scaling laws                                 0   \n",
       "Transformer (general)                                        1   \n",
       "\n",
       "                                               ERNIE 3.0 Titan  \\\n",
       "Algorithmic innovation                                           \n",
       "Link                          https://arxiv.org/abs/2112.12731   \n",
       "Included                                                     1   \n",
       "Kaplan et al. scaling laws                                   0   \n",
       "Hoffmann et al. scaling laws                                 0   \n",
       "Transformer (general)                                        1   \n",
       "\n",
       "                                                                  LLaMA 2 (70B)  \\\n",
       "Algorithmic innovation                                                            \n",
       "Link                          https://ai.meta.com/research/publications/llam...   \n",
       "Included                                                                      0   \n",
       "Kaplan et al. scaling laws                                                    0   \n",
       "Hoffmann et al. scaling laws                                                  1   \n",
       "Transformer (general)                                                         1   \n",
       "\n",
       "                                                 Gopher (280B)  \\\n",
       "Algorithmic innovation                                           \n",
       "Link                          https://arxiv.org/abs/2112.11446   \n",
       "Included                                                     1   \n",
       "Kaplan et al. scaling laws                                   1   \n",
       "Hoffmann et al. scaling laws                                 0   \n",
       "Transformer (general)                                        1   \n",
       "\n",
       "                                              Chinchilla (70B) PanGu-Σ  ...  \\\n",
       "Algorithmic innovation                                                  ...   \n",
       "Link                          https://arxiv.org/abs/2203.15556     NaN  ...   \n",
       "Included                                                     1       1  ...   \n",
       "Kaplan et al. scaling laws                                   0       0  ...   \n",
       "Hoffmann et al. scaling laws                                 1       0  ...   \n",
       "Transformer (general)                                        1       1  ...   \n",
       "\n",
       "                             Falcon-40B YaLM ALIGN  \\\n",
       "Algorithmic innovation                               \n",
       "Link                                NaN  NaN   NaN   \n",
       "Included                            0.0  0.0   0.0   \n",
       "Kaplan et al. scaling laws          NaN  NaN   NaN   \n",
       "Hoffmann et al. scaling laws        NaN  NaN   NaN   \n",
       "Transformer (general)               NaN  NaN   NaN   \n",
       "\n",
       "                                                   AlexaTM 20B  \\\n",
       "Algorithmic innovation                                           \n",
       "Link                          https://arxiv.org/abs/2208.01448   \n",
       "Included                                                     0   \n",
       "Kaplan et al. scaling laws                                   0   \n",
       "Hoffmann et al. scaling laws                                 1   \n",
       "Transformer (general)                                        1   \n",
       "\n",
       "                                                         BLOOM (176B)  \\\n",
       "Algorithmic innovation                                                  \n",
       "Link                          https://huggingface.co/bigscience/bloom   \n",
       "Included                                                            0   \n",
       "Kaplan et al. scaling laws                                          1   \n",
       "Hoffmann et al. scaling laws                                        0   \n",
       "Transformer (general)                                               1   \n",
       "\n",
       "                                                                           NLLB  \\\n",
       "Algorithmic innovation                                                            \n",
       "Link                          https://research.facebook.com/publications/no-...   \n",
       "Included                                                                      0   \n",
       "Kaplan et al. scaling laws                                                    0   \n",
       "Hoffmann et al. scaling laws                                                  0   \n",
       "Transformer (general)                                                         1   \n",
       "\n",
       "                                                   Megatron-LM  \\\n",
       "Algorithmic innovation                                           \n",
       "Link                          https://arxiv.org/abs/1909.08053   \n",
       "Included                                                     0   \n",
       "Kaplan et al. scaling laws                                 NaN   \n",
       "Hoffmann et al. scaling laws                               NaN   \n",
       "Transformer (general)                                      NaN   \n",
       "\n",
       "                                                                          GPT-2  \\\n",
       "Algorithmic innovation                                                            \n",
       "Link                          https://cdn.openai.com/better-language-models/...   \n",
       "Included                                                                      0   \n",
       "Kaplan et al. scaling laws                                                    0   \n",
       "Hoffmann et al. scaling laws                                                  0   \n",
       "Transformer (general)                                                         1   \n",
       "\n",
       "                                                                            GPT  \\\n",
       "Algorithmic innovation                                                            \n",
       "Link                          https://cdn.openai.com/research-covers/languag...   \n",
       "Included                                                                      0   \n",
       "Kaplan et al. scaling laws                                                    0   \n",
       "Hoffmann et al. scaling laws                                                  0   \n",
       "Transformer (general)                                                         1   \n",
       "\n",
       "                                                                    Transformer  \n",
       "Algorithmic innovation                                                           \n",
       "Link                          https://proceedings.neurips.cc/paper_files/pap...  \n",
       "Included                                                                      0  \n",
       "Kaplan et al. scaling laws                                                    0  \n",
       "Hoffmann et al. scaling laws                                                  0  \n",
       "Transformer (general)                                                         1  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occurrences_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GPT-4</th>\n",
       "      <th>PaLM 2</th>\n",
       "      <th>GPT-3.5</th>\n",
       "      <th>PaLM (540B)</th>\n",
       "      <th>Megatron-Turing NLG (530B)</th>\n",
       "      <th>ERNIE 3.0 Titan</th>\n",
       "      <th>LLaMA 2 (70B)</th>\n",
       "      <th>Gopher (280B)</th>\n",
       "      <th>Chinchilla (70B)</th>\n",
       "      <th>PanGu-Σ</th>\n",
       "      <th>...</th>\n",
       "      <th>Falcon-40B</th>\n",
       "      <th>YaLM</th>\n",
       "      <th>ALIGN</th>\n",
       "      <th>AlexaTM 20B</th>\n",
       "      <th>BLOOM (176B)</th>\n",
       "      <th>NLLB</th>\n",
       "      <th>Megatron-LM</th>\n",
       "      <th>GPT-2</th>\n",
       "      <th>GPT</th>\n",
       "      <th>Transformer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algorithmic innovation</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Link</th>\n",
       "      <td>https://arxiv.org/abs/2303.08774</td>\n",
       "      <td>https://ai.google/static/documents/palm2techre...</td>\n",
       "      <td>https://platform.openai.com/docs/models/gpt-3-5</td>\n",
       "      <td>https://arxiv.org/abs/2204.02311</td>\n",
       "      <td>https://arxiv.org/abs/2201.11990</td>\n",
       "      <td>https://arxiv.org/abs/2112.12731</td>\n",
       "      <td>https://ai.meta.com/research/publications/llam...</td>\n",
       "      <td>https://arxiv.org/abs/2112.11446</td>\n",
       "      <td>https://arxiv.org/abs/2203.15556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2208.01448</td>\n",
       "      <td>https://huggingface.co/bigscience/bloom</td>\n",
       "      <td>https://research.facebook.com/publications/no-...</td>\n",
       "      <td>https://arxiv.org/abs/1909.08053</td>\n",
       "      <td>https://cdn.openai.com/better-language-models/...</td>\n",
       "      <td>https://cdn.openai.com/research-covers/languag...</td>\n",
       "      <td>https://proceedings.neurips.cc/paper_files/pap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Included</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kaplan et al. scaling laws</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hoffmann et al. scaling laws</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transformer (general)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sparse Attention</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Attention</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention with locality-sensitive hashing</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi-Query Attention</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grouped Query Attention</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LayerNorm</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pre-normalization</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Learnable position embeddings</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sinusoidal position embeddings</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Relative position embeddings</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rotary position embeddings</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALiBi positional encodings</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU activation</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GELU activation</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SwiGLU activation</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sparsely-Gated Mixture-of-Experts layer (MoE)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Encoder-decoder Transformer</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Causal decoder Transformer (decoder-only)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prefix decoder Transformer (decoder-only)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Language modeling task (with Transformer architecture)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cloze task (with Transformer architecture)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Denoising autoencoding task (with Transformer architecture)</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mixture-of-Denoisers task</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dynamic batch size</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Learning rate decay</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Learning rate warmup</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inverse square root learning rate decay schedule</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear learning rate decay schedule</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cosine learning rate decay schedule</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adam optimizer</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdamW optimizer</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adafactor optimizer</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropout</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weight decay</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient clipping</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FlashAttention</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mixed precision training</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Instruction tuning</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RLHF</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PPO</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2C</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prompting for in-context learning</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chain-of-thought</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               GPT-4  \\\n",
       "Algorithmic innovation                                                                 \n",
       "Link                                                https://arxiv.org/abs/2303.08774   \n",
       "Included                                                                           0   \n",
       "Kaplan et al. scaling laws                                                       NaN   \n",
       "Hoffmann et al. scaling laws                                                     NaN   \n",
       "Transformer (general)                                                            NaN   \n",
       "Sparse Attention                                                                 NaN   \n",
       "Linear Attention                                                                 NaN   \n",
       "Attention with locality-sensitive hashing                                        NaN   \n",
       "Multi-Query Attention                                                            NaN   \n",
       "Grouped Query Attention                                                          NaN   \n",
       "RMSNorm                                                                          NaN   \n",
       "LayerNorm                                                                        NaN   \n",
       "Pre-normalization                                                                NaN   \n",
       "Learnable position embeddings                                                    NaN   \n",
       "Sinusoidal position embeddings                                                   NaN   \n",
       "Relative position embeddings                                                     NaN   \n",
       "Rotary position embeddings                                                       NaN   \n",
       "ALiBi positional encodings                                                       NaN   \n",
       "ReLU activation                                                                  NaN   \n",
       "GELU activation                                                                  NaN   \n",
       "SwiGLU activation                                                                NaN   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                    NaN   \n",
       "Encoder-decoder Transformer                                                      NaN   \n",
       "Causal decoder Transformer (decoder-only)                                        NaN   \n",
       "Prefix decoder Transformer (decoder-only)                                        NaN   \n",
       "Language modeling task (with Transformer archit...                               NaN   \n",
       "Cloze task (with Transformer architecture)                                       NaN   \n",
       "Denoising autoencoding task (with Transformer a...                               NaN   \n",
       "Mixture-of-Denoisers task                                                        NaN   \n",
       "Dynamic batch size                                                               NaN   \n",
       "Learning rate decay                                                              NaN   \n",
       "Learning rate warmup                                                             NaN   \n",
       "Inverse square root learning rate decay schedule                                 NaN   \n",
       "Linear learning rate decay schedule                                              NaN   \n",
       "Cosine learning rate decay schedule                                              NaN   \n",
       "Adam optimizer                                                                   NaN   \n",
       "AdamW optimizer                                                                  NaN   \n",
       "Adafactor optimizer                                                              NaN   \n",
       "Dropout                                                                          NaN   \n",
       "Weight decay                                                                     NaN   \n",
       "Gradient clipping                                                                NaN   \n",
       "FlashAttention                                                                   NaN   \n",
       "Mixed precision training                                                         NaN   \n",
       "Instruction tuning                                                               NaN   \n",
       "RLHF                                                                             NaN   \n",
       "PPO                                                                              NaN   \n",
       "A2C                                                                              NaN   \n",
       "Prompting for in-context learning                                                NaN   \n",
       "Chain-of-thought                                                                 NaN   \n",
       "\n",
       "                                                                                               PaLM 2  \\\n",
       "Algorithmic innovation                                                                                  \n",
       "Link                                                https://ai.google/static/documents/palm2techre...   \n",
       "Included                                                                                            0   \n",
       "Kaplan et al. scaling laws                                                                          0   \n",
       "Hoffmann et al. scaling laws                                                                        1   \n",
       "Transformer (general)                                                                               1   \n",
       "Sparse Attention                                                                                  NaN   \n",
       "Linear Attention                                                                                  NaN   \n",
       "Attention with locality-sensitive hashing                                                         NaN   \n",
       "Multi-Query Attention                                                                             NaN   \n",
       "Grouped Query Attention                                                                           NaN   \n",
       "RMSNorm                                                                                             0   \n",
       "LayerNorm                                                                                           1   \n",
       "Pre-normalization                                                                                   0   \n",
       "Learnable position embeddings                                                                     NaN   \n",
       "Sinusoidal position embeddings                                                                    NaN   \n",
       "Relative position embeddings                                                                        0   \n",
       "Rotary position embeddings                                                                        NaN   \n",
       "ALiBi positional encodings                                                                        NaN   \n",
       "ReLU activation                                                                                     0   \n",
       "GELU activation                                                                                     0   \n",
       "SwiGLU activation                                                                                   1   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                                       0   \n",
       "Encoder-decoder Transformer                                                                         0   \n",
       "Causal decoder Transformer (decoder-only)                                                           1   \n",
       "Prefix decoder Transformer (decoder-only)                                                           0   \n",
       "Language modeling task (with Transformer archit...                                                  1   \n",
       "Cloze task (with Transformer architecture)                                                          1   \n",
       "Denoising autoencoding task (with Transformer a...                                                NaN   \n",
       "Mixture-of-Denoisers task                                                                           1   \n",
       "Dynamic batch size                                                                                NaN   \n",
       "Learning rate decay                                                                               NaN   \n",
       "Learning rate warmup                                                                              NaN   \n",
       "Inverse square root learning rate decay schedule                                                  NaN   \n",
       "Linear learning rate decay schedule                                                               NaN   \n",
       "Cosine learning rate decay schedule                                                               NaN   \n",
       "Adam optimizer                                                                                    NaN   \n",
       "AdamW optimizer                                                                                   NaN   \n",
       "Adafactor optimizer                                                                               NaN   \n",
       "Dropout                                                                                           NaN   \n",
       "Weight decay                                                                                      NaN   \n",
       "Gradient clipping                                                                                 NaN   \n",
       "FlashAttention                                                                                      0   \n",
       "Mixed precision training                                                                            0   \n",
       "Instruction tuning                                                                                  1   \n",
       "RLHF                                                                                                0   \n",
       "PPO                                                                                                 0   \n",
       "A2C                                                                                               NaN   \n",
       "Prompting for in-context learning                                                                 NaN   \n",
       "Chain-of-thought                                                                                    1   \n",
       "\n",
       "                                                                                            GPT-3.5  \\\n",
       "Algorithmic innovation                                                                                \n",
       "Link                                                https://platform.openai.com/docs/models/gpt-3-5   \n",
       "Included                                                                                          0   \n",
       "Kaplan et al. scaling laws                                                                      NaN   \n",
       "Hoffmann et al. scaling laws                                                                    NaN   \n",
       "Transformer (general)                                                                           NaN   \n",
       "Sparse Attention                                                                                NaN   \n",
       "Linear Attention                                                                                NaN   \n",
       "Attention with locality-sensitive hashing                                                       NaN   \n",
       "Multi-Query Attention                                                                           NaN   \n",
       "Grouped Query Attention                                                                         NaN   \n",
       "RMSNorm                                                                                         NaN   \n",
       "LayerNorm                                                                                       NaN   \n",
       "Pre-normalization                                                                               NaN   \n",
       "Learnable position embeddings                                                                   NaN   \n",
       "Sinusoidal position embeddings                                                                  NaN   \n",
       "Relative position embeddings                                                                    NaN   \n",
       "Rotary position embeddings                                                                      NaN   \n",
       "ALiBi positional encodings                                                                      NaN   \n",
       "ReLU activation                                                                                 NaN   \n",
       "GELU activation                                                                                 NaN   \n",
       "SwiGLU activation                                                                               NaN   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                                   NaN   \n",
       "Encoder-decoder Transformer                                                                     NaN   \n",
       "Causal decoder Transformer (decoder-only)                                                       NaN   \n",
       "Prefix decoder Transformer (decoder-only)                                                       NaN   \n",
       "Language modeling task (with Transformer archit...                                              NaN   \n",
       "Cloze task (with Transformer architecture)                                                      NaN   \n",
       "Denoising autoencoding task (with Transformer a...                                              NaN   \n",
       "Mixture-of-Denoisers task                                                                       NaN   \n",
       "Dynamic batch size                                                                              NaN   \n",
       "Learning rate decay                                                                             NaN   \n",
       "Learning rate warmup                                                                            NaN   \n",
       "Inverse square root learning rate decay schedule                                                NaN   \n",
       "Linear learning rate decay schedule                                                             NaN   \n",
       "Cosine learning rate decay schedule                                                             NaN   \n",
       "Adam optimizer                                                                                  NaN   \n",
       "AdamW optimizer                                                                                 NaN   \n",
       "Adafactor optimizer                                                                             NaN   \n",
       "Dropout                                                                                         NaN   \n",
       "Weight decay                                                                                    NaN   \n",
       "Gradient clipping                                                                               NaN   \n",
       "FlashAttention                                                                                  NaN   \n",
       "Mixed precision training                                                                        NaN   \n",
       "Instruction tuning                                                                              NaN   \n",
       "RLHF                                                                                            NaN   \n",
       "PPO                                                                                             NaN   \n",
       "A2C                                                                                             NaN   \n",
       "Prompting for in-context learning                                                               NaN   \n",
       "Chain-of-thought                                                                                NaN   \n",
       "\n",
       "                                                                         PaLM (540B)  \\\n",
       "Algorithmic innovation                                                                 \n",
       "Link                                                https://arxiv.org/abs/2204.02311   \n",
       "Included                                                                           1   \n",
       "Kaplan et al. scaling laws                                                         1   \n",
       "Hoffmann et al. scaling laws                                                       0   \n",
       "Transformer (general)                                                              1   \n",
       "Sparse Attention                                                                   0   \n",
       "Linear Attention                                                                   0   \n",
       "Attention with locality-sensitive hashing                                          0   \n",
       "Multi-Query Attention                                                              1   \n",
       "Grouped Query Attention                                                            0   \n",
       "RMSNorm                                                                            0   \n",
       "LayerNorm                                                                          1   \n",
       "Pre-normalization                                                                  1   \n",
       "Learnable position embeddings                                                      0   \n",
       "Sinusoidal position embeddings                                                     0   \n",
       "Relative position embeddings                                                       0   \n",
       "Rotary position embeddings                                                         1   \n",
       "ALiBi positional encodings                                                         0   \n",
       "ReLU activation                                                                    0   \n",
       "GELU activation                                                                    0   \n",
       "SwiGLU activation                                                                  1   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                      0   \n",
       "Encoder-decoder Transformer                                                        0   \n",
       "Causal decoder Transformer (decoder-only)                                          1   \n",
       "Prefix decoder Transformer (decoder-only)                                          0   \n",
       "Language modeling task (with Transformer archit...                                 1   \n",
       "Cloze task (with Transformer architecture)                                         0   \n",
       "Denoising autoencoding task (with Transformer a...                                 0   \n",
       "Mixture-of-Denoisers task                                                          0   \n",
       "Dynamic batch size                                                                 1   \n",
       "Learning rate decay                                                                1   \n",
       "Learning rate warmup                                                               0   \n",
       "Inverse square root learning rate decay schedule                                   1   \n",
       "Linear learning rate decay schedule                                                0   \n",
       "Cosine learning rate decay schedule                                                0   \n",
       "Adam optimizer                                                                     0   \n",
       "AdamW optimizer                                                                    0   \n",
       "Adafactor optimizer                                                                1   \n",
       "Dropout                                                                            0   \n",
       "Weight decay                                                                       1   \n",
       "Gradient clipping                                                                  1   \n",
       "FlashAttention                                                                     0   \n",
       "Mixed precision training                                                           0   \n",
       "Instruction tuning                                                                 1   \n",
       "RLHF                                                                               0   \n",
       "PPO                                                                                0   \n",
       "A2C                                                                                0   \n",
       "Prompting for in-context learning                                                  1   \n",
       "Chain-of-thought                                                                   1   \n",
       "\n",
       "                                                          Megatron-Turing NLG (530B)  \\\n",
       "Algorithmic innovation                                                                 \n",
       "Link                                                https://arxiv.org/abs/2201.11990   \n",
       "Included                                                                           1   \n",
       "Kaplan et al. scaling laws                                                         1   \n",
       "Hoffmann et al. scaling laws                                                       0   \n",
       "Transformer (general)                                                              1   \n",
       "Sparse Attention                                                                   0   \n",
       "Linear Attention                                                                   0   \n",
       "Attention with locality-sensitive hashing                                          0   \n",
       "Multi-Query Attention                                                              0   \n",
       "Grouped Query Attention                                                            0   \n",
       "RMSNorm                                                                            0   \n",
       "LayerNorm                                                                          1   \n",
       "Pre-normalization                                                                  1   \n",
       "Learnable position embeddings                                                      1   \n",
       "Sinusoidal position embeddings                                                     0   \n",
       "Relative position embeddings                                                       0   \n",
       "Rotary position embeddings                                                         0   \n",
       "ALiBi positional encodings                                                         0   \n",
       "ReLU activation                                                                    0   \n",
       "GELU activation                                                                    1   \n",
       "SwiGLU activation                                                                  0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                      0   \n",
       "Encoder-decoder Transformer                                                        0   \n",
       "Causal decoder Transformer (decoder-only)                                          1   \n",
       "Prefix decoder Transformer (decoder-only)                                          0   \n",
       "Language modeling task (with Transformer archit...                                 1   \n",
       "Cloze task (with Transformer architecture)                                         0   \n",
       "Denoising autoencoding task (with Transformer a...                                 0   \n",
       "Mixture-of-Denoisers task                                                          0   \n",
       "Dynamic batch size                                                                 1   \n",
       "Learning rate decay                                                                1   \n",
       "Learning rate warmup                                                               1   \n",
       "Inverse square root learning rate decay schedule                                   0   \n",
       "Linear learning rate decay schedule                                                0   \n",
       "Cosine learning rate decay schedule                                                1   \n",
       "Adam optimizer                                                                     1   \n",
       "AdamW optimizer                                                                    0   \n",
       "Adafactor optimizer                                                                0   \n",
       "Dropout                                                                            0   \n",
       "Weight decay                                                                       1   \n",
       "Gradient clipping                                                                  1   \n",
       "FlashAttention                                                                     0   \n",
       "Mixed precision training                                                           1   \n",
       "Instruction tuning                                                                 0   \n",
       "RLHF                                                                               0   \n",
       "PPO                                                                                0   \n",
       "A2C                                                                                0   \n",
       "Prompting for in-context learning                                                  1   \n",
       "Chain-of-thought                                                                   0   \n",
       "\n",
       "                                                                     ERNIE 3.0 Titan  \\\n",
       "Algorithmic innovation                                                                 \n",
       "Link                                                https://arxiv.org/abs/2112.12731   \n",
       "Included                                                                           1   \n",
       "Kaplan et al. scaling laws                                                         0   \n",
       "Hoffmann et al. scaling laws                                                       0   \n",
       "Transformer (general)                                                              1   \n",
       "Sparse Attention                                                                   0   \n",
       "Linear Attention                                                                   0   \n",
       "Attention with locality-sensitive hashing                                          0   \n",
       "Multi-Query Attention                                                              0   \n",
       "Grouped Query Attention                                                            0   \n",
       "RMSNorm                                                                            0   \n",
       "LayerNorm                                                                          1   \n",
       "Pre-normalization                                                                  0   \n",
       "Learnable position embeddings                                                      0   \n",
       "Sinusoidal position embeddings                                                     0   \n",
       "Relative position embeddings                                                       1   \n",
       "Rotary position embeddings                                                         0   \n",
       "ALiBi positional encodings                                                         0   \n",
       "ReLU activation                                                                    1   \n",
       "GELU activation                                                                    0   \n",
       "SwiGLU activation                                                                  0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                      0   \n",
       "Encoder-decoder Transformer                                                        1   \n",
       "Causal decoder Transformer (decoder-only)                                          0   \n",
       "Prefix decoder Transformer (decoder-only)                                          0   \n",
       "Language modeling task (with Transformer archit...                                 1   \n",
       "Cloze task (with Transformer architecture)                                         1   \n",
       "Denoising autoencoding task (with Transformer a...                                 0   \n",
       "Mixture-of-Denoisers task                                                          0   \n",
       "Dynamic batch size                                                                 0   \n",
       "Learning rate decay                                                                1   \n",
       "Learning rate warmup                                                               1   \n",
       "Inverse square root learning rate decay schedule                                   0   \n",
       "Linear learning rate decay schedule                                                1   \n",
       "Cosine learning rate decay schedule                                                0   \n",
       "Adam optimizer                                                                     1   \n",
       "AdamW optimizer                                                                    0   \n",
       "Adafactor optimizer                                                                0   \n",
       "Dropout                                                                            0   \n",
       "Weight decay                                                                       1   \n",
       "Gradient clipping                                                                  1   \n",
       "FlashAttention                                                                     0   \n",
       "Mixed precision training                                                           0   \n",
       "Instruction tuning                                                                 0   \n",
       "RLHF                                                                               0   \n",
       "PPO                                                                                0   \n",
       "A2C                                                                                0   \n",
       "Prompting for in-context learning                                                  1   \n",
       "Chain-of-thought                                                                   0   \n",
       "\n",
       "                                                                                        LLaMA 2 (70B)  \\\n",
       "Algorithmic innovation                                                                                  \n",
       "Link                                                https://ai.meta.com/research/publications/llam...   \n",
       "Included                                                                                            0   \n",
       "Kaplan et al. scaling laws                                                                          0   \n",
       "Hoffmann et al. scaling laws                                                                        1   \n",
       "Transformer (general)                                                                               1   \n",
       "Sparse Attention                                                                                    0   \n",
       "Linear Attention                                                                                    0   \n",
       "Attention with locality-sensitive hashing                                                           0   \n",
       "Multi-Query Attention                                                                               0   \n",
       "Grouped Query Attention                                                                             1   \n",
       "RMSNorm                                                                                             1   \n",
       "LayerNorm                                                                                           0   \n",
       "Pre-normalization                                                                                   1   \n",
       "Learnable position embeddings                                                                       0   \n",
       "Sinusoidal position embeddings                                                                      0   \n",
       "Relative position embeddings                                                                        0   \n",
       "Rotary position embeddings                                                                          1   \n",
       "ALiBi positional encodings                                                                          0   \n",
       "ReLU activation                                                                                     0   \n",
       "GELU activation                                                                                     0   \n",
       "SwiGLU activation                                                                                   1   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                                       0   \n",
       "Encoder-decoder Transformer                                                                         1   \n",
       "Causal decoder Transformer (decoder-only)                                                           0   \n",
       "Prefix decoder Transformer (decoder-only)                                                           0   \n",
       "Language modeling task (with Transformer archit...                                                  1   \n",
       "Cloze task (with Transformer architecture)                                                          0   \n",
       "Denoising autoencoding task (with Transformer a...                                                  0   \n",
       "Mixture-of-Denoisers task                                                                           0   \n",
       "Dynamic batch size                                                                                  0   \n",
       "Learning rate decay                                                                                 1   \n",
       "Learning rate warmup                                                                                1   \n",
       "Inverse square root learning rate decay schedule                                                    0   \n",
       "Linear learning rate decay schedule                                                                 0   \n",
       "Cosine learning rate decay schedule                                                                 1   \n",
       "Adam optimizer                                                                                      0   \n",
       "AdamW optimizer                                                                                     1   \n",
       "Adafactor optimizer                                                                                 0   \n",
       "Dropout                                                                                             0   \n",
       "Weight decay                                                                                        1   \n",
       "Gradient clipping                                                                                   1   \n",
       "FlashAttention                                                                                      1   \n",
       "Mixed precision training                                                                            0   \n",
       "Instruction tuning                                                                                  1   \n",
       "RLHF                                                                                                1   \n",
       "PPO                                                                                                 1   \n",
       "A2C                                                                                                 0   \n",
       "Prompting for in-context learning                                                                   1   \n",
       "Chain-of-thought                                                                                    0   \n",
       "\n",
       "                                                                       Gopher (280B)  \\\n",
       "Algorithmic innovation                                                                 \n",
       "Link                                                https://arxiv.org/abs/2112.11446   \n",
       "Included                                                                           1   \n",
       "Kaplan et al. scaling laws                                                         1   \n",
       "Hoffmann et al. scaling laws                                                       0   \n",
       "Transformer (general)                                                              1   \n",
       "Sparse Attention                                                                   0   \n",
       "Linear Attention                                                                   0   \n",
       "Attention with locality-sensitive hashing                                          0   \n",
       "Multi-Query Attention                                                              0   \n",
       "Grouped Query Attention                                                            0   \n",
       "RMSNorm                                                                            1   \n",
       "LayerNorm                                                                          0   \n",
       "Pre-normalization                                                                  1   \n",
       "Learnable position embeddings                                                      0   \n",
       "Sinusoidal position embeddings                                                     0   \n",
       "Relative position embeddings                                                       1   \n",
       "Rotary position embeddings                                                         0   \n",
       "ALiBi positional encodings                                                         0   \n",
       "ReLU activation                                                                    0   \n",
       "GELU activation                                                                    1   \n",
       "SwiGLU activation                                                                  0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                      0   \n",
       "Encoder-decoder Transformer                                                        0   \n",
       "Causal decoder Transformer (decoder-only)                                          1   \n",
       "Prefix decoder Transformer (decoder-only)                                          0   \n",
       "Language modeling task (with Transformer archit...                                 1   \n",
       "Cloze task (with Transformer architecture)                                         0   \n",
       "Denoising autoencoding task (with Transformer a...                                 0   \n",
       "Mixture-of-Denoisers task                                                          0   \n",
       "Dynamic batch size                                                                 1   \n",
       "Learning rate decay                                                                1   \n",
       "Learning rate warmup                                                               1   \n",
       "Inverse square root learning rate decay schedule                                   0   \n",
       "Linear learning rate decay schedule                                                0   \n",
       "Cosine learning rate decay schedule                                                1   \n",
       "Adam optimizer                                                                     1   \n",
       "AdamW optimizer                                                                    0   \n",
       "Adafactor optimizer                                                                0   \n",
       "Dropout                                                                            1   \n",
       "Weight decay                                                                       1   \n",
       "Gradient clipping                                                                  1   \n",
       "FlashAttention                                                                     0   \n",
       "Mixed precision training                                                           0   \n",
       "Instruction tuning                                                                 0   \n",
       "RLHF                                                                               0   \n",
       "PPO                                                                                0   \n",
       "A2C                                                                                0   \n",
       "Prompting for in-context learning                                                  1   \n",
       "Chain-of-thought                                                                   0   \n",
       "\n",
       "                                                                    Chinchilla (70B)  \\\n",
       "Algorithmic innovation                                                                 \n",
       "Link                                                https://arxiv.org/abs/2203.15556   \n",
       "Included                                                                           1   \n",
       "Kaplan et al. scaling laws                                                         0   \n",
       "Hoffmann et al. scaling laws                                                       1   \n",
       "Transformer (general)                                                              1   \n",
       "Sparse Attention                                                                   0   \n",
       "Linear Attention                                                                   0   \n",
       "Attention with locality-sensitive hashing                                          0   \n",
       "Multi-Query Attention                                                              0   \n",
       "Grouped Query Attention                                                            0   \n",
       "RMSNorm                                                                            1   \n",
       "LayerNorm                                                                          0   \n",
       "Pre-normalization                                                                  1   \n",
       "Learnable position embeddings                                                      0   \n",
       "Sinusoidal position embeddings                                                     0   \n",
       "Relative position embeddings                                                       1   \n",
       "Rotary position embeddings                                                         0   \n",
       "ALiBi positional encodings                                                         0   \n",
       "ReLU activation                                                                    0   \n",
       "GELU activation                                                                    1   \n",
       "SwiGLU activation                                                                  0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                      0   \n",
       "Encoder-decoder Transformer                                                        0   \n",
       "Causal decoder Transformer (decoder-only)                                          1   \n",
       "Prefix decoder Transformer (decoder-only)                                          0   \n",
       "Language modeling task (with Transformer archit...                                 1   \n",
       "Cloze task (with Transformer architecture)                                         0   \n",
       "Denoising autoencoding task (with Transformer a...                                 0   \n",
       "Mixture-of-Denoisers task                                                          0   \n",
       "Dynamic batch size                                                                 1   \n",
       "Learning rate decay                                                                1   \n",
       "Learning rate warmup                                                               1   \n",
       "Inverse square root learning rate decay schedule                                   0   \n",
       "Linear learning rate decay schedule                                                0   \n",
       "Cosine learning rate decay schedule                                                1   \n",
       "Adam optimizer                                                                     0   \n",
       "AdamW optimizer                                                                    1   \n",
       "Adafactor optimizer                                                                0   \n",
       "Dropout                                                                            1   \n",
       "Weight decay                                                                       1   \n",
       "Gradient clipping                                                                  1   \n",
       "FlashAttention                                                                     0   \n",
       "Mixed precision training                                                           0   \n",
       "Instruction tuning                                                                 1   \n",
       "RLHF                                                                               1   \n",
       "PPO                                                                                0   \n",
       "A2C                                                                                1   \n",
       "Prompting for in-context learning                                                  1   \n",
       "Chain-of-thought                                                                   0   \n",
       "\n",
       "                                                   PanGu-Σ  ... Falcon-40B  \\\n",
       "Algorithmic innovation                                      ...              \n",
       "Link                                                   NaN  ...        NaN   \n",
       "Included                                                 1  ...        0.0   \n",
       "Kaplan et al. scaling laws                               0  ...        NaN   \n",
       "Hoffmann et al. scaling laws                             0  ...        NaN   \n",
       "Transformer (general)                                    1  ...        NaN   \n",
       "Sparse Attention                                         0  ...        NaN   \n",
       "Linear Attention                                         0  ...        NaN   \n",
       "Attention with locality-sensitive hashing                0  ...        NaN   \n",
       "Multi-Query Attention                                    0  ...        NaN   \n",
       "Grouped Query Attention                                  0  ...        NaN   \n",
       "RMSNorm                                                  0  ...        NaN   \n",
       "LayerNorm                                                1  ...        NaN   \n",
       "Pre-normalization                                        1  ...        NaN   \n",
       "Learnable position embeddings                            1  ...        NaN   \n",
       "Sinusoidal position embeddings                           0  ...        NaN   \n",
       "Relative position embeddings                             0  ...        NaN   \n",
       "Rotary position embeddings                               0  ...        NaN   \n",
       "ALiBi positional encodings                               0  ...        NaN   \n",
       "ReLU activation                                          0  ...        NaN   \n",
       "GELU activation                                          1  ...        NaN   \n",
       "SwiGLU activation                                        0  ...        NaN   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)            1  ...        NaN   \n",
       "Encoder-decoder Transformer                              0  ...        NaN   \n",
       "Causal decoder Transformer (decoder-only)                1  ...        NaN   \n",
       "Prefix decoder Transformer (decoder-only)                0  ...        NaN   \n",
       "Language modeling task (with Transformer archit...       1  ...        NaN   \n",
       "Cloze task (with Transformer architecture)               0  ...        NaN   \n",
       "Denoising autoencoding task (with Transformer a...       0  ...        NaN   \n",
       "Mixture-of-Denoisers task                                0  ...        NaN   \n",
       "Dynamic batch size                                       0  ...        NaN   \n",
       "Learning rate decay                                      1  ...        NaN   \n",
       "Learning rate warmup                                     1  ...        NaN   \n",
       "Inverse square root learning rate decay schedule         0  ...        NaN   \n",
       "Linear learning rate decay schedule                      0  ...        NaN   \n",
       "Cosine learning rate decay schedule                      0  ...        NaN   \n",
       "Adam optimizer                                           1  ...        NaN   \n",
       "AdamW optimizer                                          0  ...        NaN   \n",
       "Adafactor optimizer                                      0  ...        NaN   \n",
       "Dropout                                                  0  ...        NaN   \n",
       "Weight decay                                             1  ...        NaN   \n",
       "Gradient clipping                                        0  ...        NaN   \n",
       "FlashAttention                                           0  ...        NaN   \n",
       "Mixed precision training                                 1  ...        NaN   \n",
       "Instruction tuning                                       0  ...        NaN   \n",
       "RLHF                                                     0  ...        NaN   \n",
       "PPO                                                      0  ...        NaN   \n",
       "A2C                                                      0  ...        NaN   \n",
       "Prompting for in-context learning                        1  ...        NaN   \n",
       "Chain-of-thought                                         0  ...        NaN   \n",
       "\n",
       "                                                   YaLM ALIGN  \\\n",
       "Algorithmic innovation                                          \n",
       "Link                                                NaN   NaN   \n",
       "Included                                            0.0   0.0   \n",
       "Kaplan et al. scaling laws                          NaN   NaN   \n",
       "Hoffmann et al. scaling laws                        NaN   NaN   \n",
       "Transformer (general)                               NaN   NaN   \n",
       "Sparse Attention                                    NaN   NaN   \n",
       "Linear Attention                                    NaN   NaN   \n",
       "Attention with locality-sensitive hashing           NaN   NaN   \n",
       "Multi-Query Attention                               NaN   NaN   \n",
       "Grouped Query Attention                             NaN   NaN   \n",
       "RMSNorm                                             NaN   NaN   \n",
       "LayerNorm                                           NaN   NaN   \n",
       "Pre-normalization                                   NaN   NaN   \n",
       "Learnable position embeddings                       NaN   NaN   \n",
       "Sinusoidal position embeddings                      NaN   NaN   \n",
       "Relative position embeddings                        NaN   NaN   \n",
       "Rotary position embeddings                          NaN   NaN   \n",
       "ALiBi positional encodings                          NaN   NaN   \n",
       "ReLU activation                                     NaN   NaN   \n",
       "GELU activation                                     NaN   NaN   \n",
       "SwiGLU activation                                   NaN   NaN   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)       NaN   NaN   \n",
       "Encoder-decoder Transformer                         NaN   NaN   \n",
       "Causal decoder Transformer (decoder-only)           NaN   NaN   \n",
       "Prefix decoder Transformer (decoder-only)           NaN   NaN   \n",
       "Language modeling task (with Transformer archit...  NaN   NaN   \n",
       "Cloze task (with Transformer architecture)          NaN   NaN   \n",
       "Denoising autoencoding task (with Transformer a...  NaN   NaN   \n",
       "Mixture-of-Denoisers task                           NaN   NaN   \n",
       "Dynamic batch size                                  NaN   NaN   \n",
       "Learning rate decay                                 NaN   NaN   \n",
       "Learning rate warmup                                NaN   NaN   \n",
       "Inverse square root learning rate decay schedule    NaN   NaN   \n",
       "Linear learning rate decay schedule                 NaN   NaN   \n",
       "Cosine learning rate decay schedule                 NaN   NaN   \n",
       "Adam optimizer                                      NaN   NaN   \n",
       "AdamW optimizer                                     NaN   NaN   \n",
       "Adafactor optimizer                                 NaN   NaN   \n",
       "Dropout                                             NaN   NaN   \n",
       "Weight decay                                        NaN   NaN   \n",
       "Gradient clipping                                   NaN   NaN   \n",
       "FlashAttention                                      NaN   NaN   \n",
       "Mixed precision training                            NaN   NaN   \n",
       "Instruction tuning                                  NaN   NaN   \n",
       "RLHF                                                NaN   NaN   \n",
       "PPO                                                 NaN   NaN   \n",
       "A2C                                                 NaN   NaN   \n",
       "Prompting for in-context learning                   NaN   NaN   \n",
       "Chain-of-thought                                    NaN   NaN   \n",
       "\n",
       "                                                                         AlexaTM 20B  \\\n",
       "Algorithmic innovation                                                                 \n",
       "Link                                                https://arxiv.org/abs/2208.01448   \n",
       "Included                                                                           0   \n",
       "Kaplan et al. scaling laws                                                         0   \n",
       "Hoffmann et al. scaling laws                                                       1   \n",
       "Transformer (general)                                                              1   \n",
       "Sparse Attention                                                                   0   \n",
       "Linear Attention                                                                   0   \n",
       "Attention with locality-sensitive hashing                                        NaN   \n",
       "Multi-Query Attention                                                            NaN   \n",
       "Grouped Query Attention                                                          NaN   \n",
       "RMSNorm                                                                            0   \n",
       "LayerNorm                                                                          1   \n",
       "Pre-normalization                                                                  1   \n",
       "Learnable position embeddings                                                      1   \n",
       "Sinusoidal position embeddings                                                     0   \n",
       "Relative position embeddings                                                       0   \n",
       "Rotary position embeddings                                                         0   \n",
       "ALiBi positional encodings                                                         0   \n",
       "ReLU activation                                                                    1   \n",
       "GELU activation                                                                    0   \n",
       "SwiGLU activation                                                                  0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                      0   \n",
       "Encoder-decoder Transformer                                                      NaN   \n",
       "Causal decoder Transformer (decoder-only)                                          0   \n",
       "Prefix decoder Transformer (decoder-only)                                        NaN   \n",
       "Language modeling task (with Transformer archit...                                 1   \n",
       "Cloze task (with Transformer architecture)                                         1   \n",
       "Denoising autoencoding task (with Transformer a...                               NaN   \n",
       "Mixture-of-Denoisers task                                                        NaN   \n",
       "Dynamic batch size                                                                 0   \n",
       "Learning rate decay                                                                1   \n",
       "Learning rate warmup                                                             NaN   \n",
       "Inverse square root learning rate decay schedule                                 NaN   \n",
       "Linear learning rate decay schedule                                                1   \n",
       "Cosine learning rate decay schedule                                                0   \n",
       "Adam optimizer                                                                     1   \n",
       "AdamW optimizer                                                                    0   \n",
       "Adafactor optimizer                                                              NaN   \n",
       "Dropout                                                                            0   \n",
       "Weight decay                                                                       1   \n",
       "Gradient clipping                                                                  0   \n",
       "FlashAttention                                                                     0   \n",
       "Mixed precision training                                                           0   \n",
       "Instruction tuning                                                                 0   \n",
       "RLHF                                                                               0   \n",
       "PPO                                                                                0   \n",
       "A2C                                                                              NaN   \n",
       "Prompting for in-context learning                                                NaN   \n",
       "Chain-of-thought                                                                   1   \n",
       "\n",
       "                                                                               BLOOM (176B)  \\\n",
       "Algorithmic innovation                                                                        \n",
       "Link                                                https://huggingface.co/bigscience/bloom   \n",
       "Included                                                                                  0   \n",
       "Kaplan et al. scaling laws                                                                1   \n",
       "Hoffmann et al. scaling laws                                                              0   \n",
       "Transformer (general)                                                                     1   \n",
       "Sparse Attention                                                                          0   \n",
       "Linear Attention                                                                          0   \n",
       "Attention with locality-sensitive hashing                                               NaN   \n",
       "Multi-Query Attention                                                                   NaN   \n",
       "Grouped Query Attention                                                                 NaN   \n",
       "RMSNorm                                                                                   0   \n",
       "LayerNorm                                                                                 1   \n",
       "Pre-normalization                                                                         1   \n",
       "Learnable position embeddings                                                             0   \n",
       "Sinusoidal position embeddings                                                            0   \n",
       "Relative position embeddings                                                              0   \n",
       "Rotary position embeddings                                                                0   \n",
       "ALiBi positional encodings                                                                1   \n",
       "ReLU activation                                                                           0   \n",
       "GELU activation                                                                           1   \n",
       "SwiGLU activation                                                                         0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                             0   \n",
       "Encoder-decoder Transformer                                                             NaN   \n",
       "Causal decoder Transformer (decoder-only)                                                 1   \n",
       "Prefix decoder Transformer (decoder-only)                                               NaN   \n",
       "Language modeling task (with Transformer archit...                                        1   \n",
       "Cloze task (with Transformer architecture)                                                0   \n",
       "Denoising autoencoding task (with Transformer a...                                      NaN   \n",
       "Mixture-of-Denoisers task                                                               NaN   \n",
       "Dynamic batch size                                                                        0   \n",
       "Learning rate decay                                                                       1   \n",
       "Learning rate warmup                                                                    NaN   \n",
       "Inverse square root learning rate decay schedule                                        NaN   \n",
       "Linear learning rate decay schedule                                                       0   \n",
       "Cosine learning rate decay schedule                                                       1   \n",
       "Adam optimizer                                                                            1   \n",
       "AdamW optimizer                                                                           0   \n",
       "Adafactor optimizer                                                                     NaN   \n",
       "Dropout                                                                                   0   \n",
       "Weight decay                                                                              1   \n",
       "Gradient clipping                                                                         1   \n",
       "FlashAttention                                                                            0   \n",
       "Mixed precision training                                                                  1   \n",
       "Instruction tuning                                                                        0   \n",
       "RLHF                                                                                      0   \n",
       "PPO                                                                                       0   \n",
       "A2C                                                                                     NaN   \n",
       "Prompting for in-context learning                                                       NaN   \n",
       "Chain-of-thought                                                                          0   \n",
       "\n",
       "                                                                                                 NLLB  \\\n",
       "Algorithmic innovation                                                                                  \n",
       "Link                                                https://research.facebook.com/publications/no-...   \n",
       "Included                                                                                            0   \n",
       "Kaplan et al. scaling laws                                                                          0   \n",
       "Hoffmann et al. scaling laws                                                                        0   \n",
       "Transformer (general)                                                                               1   \n",
       "Sparse Attention                                                                                    0   \n",
       "Linear Attention                                                                                    0   \n",
       "Attention with locality-sensitive hashing                                                         NaN   \n",
       "Multi-Query Attention                                                                             NaN   \n",
       "Grouped Query Attention                                                                           NaN   \n",
       "RMSNorm                                                                                             0   \n",
       "LayerNorm                                                                                           1   \n",
       "Pre-normalization                                                                                   1   \n",
       "Learnable position embeddings                                                                       0   \n",
       "Sinusoidal position embeddings                                                                      1   \n",
       "Relative position embeddings                                                                        0   \n",
       "Rotary position embeddings                                                                          0   \n",
       "ALiBi positional encodings                                                                          0   \n",
       "ReLU activation                                                                                     1   \n",
       "GELU activation                                                                                     0   \n",
       "SwiGLU activation                                                                                   0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                                       1   \n",
       "Encoder-decoder Transformer                                                                       NaN   \n",
       "Causal decoder Transformer (decoder-only)                                                           0   \n",
       "Prefix decoder Transformer (decoder-only)                                                         NaN   \n",
       "Language modeling task (with Transformer archit...                                                  1   \n",
       "Cloze task (with Transformer architecture)                                                          1   \n",
       "Denoising autoencoding task (with Transformer a...                                                NaN   \n",
       "Mixture-of-Denoisers task                                                                         NaN   \n",
       "Dynamic batch size                                                                                  0   \n",
       "Learning rate decay                                                                                 1   \n",
       "Learning rate warmup                                                                                1   \n",
       "Inverse square root learning rate decay schedule                                                    1   \n",
       "Linear learning rate decay schedule                                                                 0   \n",
       "Cosine learning rate decay schedule                                                                 0   \n",
       "Adam optimizer                                                                                      1   \n",
       "AdamW optimizer                                                                                     0   \n",
       "Adafactor optimizer                                                                               NaN   \n",
       "Dropout                                                                                             1   \n",
       "Weight decay                                                                                        0   \n",
       "Gradient clipping                                                                                   0   \n",
       "FlashAttention                                                                                      0   \n",
       "Mixed precision training                                                                            0   \n",
       "Instruction tuning                                                                                  0   \n",
       "RLHF                                                                                                0   \n",
       "PPO                                                                                                 0   \n",
       "A2C                                                                                               NaN   \n",
       "Prompting for in-context learning                                                                 NaN   \n",
       "Chain-of-thought                                                                                    0   \n",
       "\n",
       "                                                                         Megatron-LM  \\\n",
       "Algorithmic innovation                                                                 \n",
       "Link                                                https://arxiv.org/abs/1909.08053   \n",
       "Included                                                                           0   \n",
       "Kaplan et al. scaling laws                                                       NaN   \n",
       "Hoffmann et al. scaling laws                                                     NaN   \n",
       "Transformer (general)                                                            NaN   \n",
       "Sparse Attention                                                                 NaN   \n",
       "Linear Attention                                                                 NaN   \n",
       "Attention with locality-sensitive hashing                                        NaN   \n",
       "Multi-Query Attention                                                            NaN   \n",
       "Grouped Query Attention                                                          NaN   \n",
       "RMSNorm                                                                          NaN   \n",
       "LayerNorm                                                                        NaN   \n",
       "Pre-normalization                                                                NaN   \n",
       "Learnable position embeddings                                                    NaN   \n",
       "Sinusoidal position embeddings                                                   NaN   \n",
       "Relative position embeddings                                                     NaN   \n",
       "Rotary position embeddings                                                       NaN   \n",
       "ALiBi positional encodings                                                       NaN   \n",
       "ReLU activation                                                                  NaN   \n",
       "GELU activation                                                                  NaN   \n",
       "SwiGLU activation                                                                NaN   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                    NaN   \n",
       "Encoder-decoder Transformer                                                      NaN   \n",
       "Causal decoder Transformer (decoder-only)                                        NaN   \n",
       "Prefix decoder Transformer (decoder-only)                                        NaN   \n",
       "Language modeling task (with Transformer archit...                               NaN   \n",
       "Cloze task (with Transformer architecture)                                       NaN   \n",
       "Denoising autoencoding task (with Transformer a...                               NaN   \n",
       "Mixture-of-Denoisers task                                                        NaN   \n",
       "Dynamic batch size                                                               NaN   \n",
       "Learning rate decay                                                              NaN   \n",
       "Learning rate warmup                                                             NaN   \n",
       "Inverse square root learning rate decay schedule                                 NaN   \n",
       "Linear learning rate decay schedule                                              NaN   \n",
       "Cosine learning rate decay schedule                                              NaN   \n",
       "Adam optimizer                                                                   NaN   \n",
       "AdamW optimizer                                                                  NaN   \n",
       "Adafactor optimizer                                                              NaN   \n",
       "Dropout                                                                          NaN   \n",
       "Weight decay                                                                     NaN   \n",
       "Gradient clipping                                                                NaN   \n",
       "FlashAttention                                                                   NaN   \n",
       "Mixed precision training                                                           1   \n",
       "Instruction tuning                                                               NaN   \n",
       "RLHF                                                                             NaN   \n",
       "PPO                                                                              NaN   \n",
       "A2C                                                                              NaN   \n",
       "Prompting for in-context learning                                                NaN   \n",
       "Chain-of-thought                                                                 NaN   \n",
       "\n",
       "                                                                                                GPT-2  \\\n",
       "Algorithmic innovation                                                                                  \n",
       "Link                                                https://cdn.openai.com/better-language-models/...   \n",
       "Included                                                                                            0   \n",
       "Kaplan et al. scaling laws                                                                          0   \n",
       "Hoffmann et al. scaling laws                                                                        0   \n",
       "Transformer (general)                                                                               1   \n",
       "Sparse Attention                                                                                    0   \n",
       "Linear Attention                                                                                    0   \n",
       "Attention with locality-sensitive hashing                                                         NaN   \n",
       "Multi-Query Attention                                                                               0   \n",
       "Grouped Query Attention                                                                             0   \n",
       "RMSNorm                                                                                             0   \n",
       "LayerNorm                                                                                           1   \n",
       "Pre-normalization                                                                                   1   \n",
       "Learnable position embeddings                                                                       1   \n",
       "Sinusoidal position embeddings                                                                      0   \n",
       "Relative position embeddings                                                                        0   \n",
       "Rotary position embeddings                                                                          0   \n",
       "ALiBi positional encodings                                                                          0   \n",
       "ReLU activation                                                                                     0   \n",
       "GELU activation                                                                                     1   \n",
       "SwiGLU activation                                                                                   0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                                       0   \n",
       "Encoder-decoder Transformer                                                                       NaN   \n",
       "Causal decoder Transformer (decoder-only)                                                           1   \n",
       "Prefix decoder Transformer (decoder-only)                                                         NaN   \n",
       "Language modeling task (with Transformer archit...                                                  1   \n",
       "Cloze task (with Transformer architecture)                                                          0   \n",
       "Denoising autoencoding task (with Transformer a...                                                NaN   \n",
       "Mixture-of-Denoisers task                                                                         NaN   \n",
       "Dynamic batch size                                                                                  0   \n",
       "Learning rate decay                                                                                 1   \n",
       "Learning rate warmup                                                                                1   \n",
       "Inverse square root learning rate decay schedule                                                    0   \n",
       "Linear learning rate decay schedule                                                                 1   \n",
       "Cosine learning rate decay schedule                                                                 1   \n",
       "Adam optimizer                                                                                      0   \n",
       "AdamW optimizer                                                                                     1   \n",
       "Adafactor optimizer                                                                                 0   \n",
       "Dropout                                                                                             1   \n",
       "Weight decay                                                                                        1   \n",
       "Gradient clipping                                                                                   0   \n",
       "FlashAttention                                                                                      0   \n",
       "Mixed precision training                                                                            0   \n",
       "Instruction tuning                                                                                  0   \n",
       "RLHF                                                                                                0   \n",
       "PPO                                                                                                 0   \n",
       "A2C                                                                                               NaN   \n",
       "Prompting for in-context learning                                                                 NaN   \n",
       "Chain-of-thought                                                                                    0   \n",
       "\n",
       "                                                                                                  GPT  \\\n",
       "Algorithmic innovation                                                                                  \n",
       "Link                                                https://cdn.openai.com/research-covers/languag...   \n",
       "Included                                                                                            0   \n",
       "Kaplan et al. scaling laws                                                                          0   \n",
       "Hoffmann et al. scaling laws                                                                        0   \n",
       "Transformer (general)                                                                               1   \n",
       "Sparse Attention                                                                                    0   \n",
       "Linear Attention                                                                                    0   \n",
       "Attention with locality-sensitive hashing                                                         NaN   \n",
       "Multi-Query Attention                                                                               0   \n",
       "Grouped Query Attention                                                                             0   \n",
       "RMSNorm                                                                                             0   \n",
       "LayerNorm                                                                                           1   \n",
       "Pre-normalization                                                                                   0   \n",
       "Learnable position embeddings                                                                       1   \n",
       "Sinusoidal position embeddings                                                                      0   \n",
       "Relative position embeddings                                                                        0   \n",
       "Rotary position embeddings                                                                          0   \n",
       "ALiBi positional encodings                                                                          0   \n",
       "ReLU activation                                                                                     0   \n",
       "GELU activation                                                                                     1   \n",
       "SwiGLU activation                                                                                   0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                                       0   \n",
       "Encoder-decoder Transformer                                                                       NaN   \n",
       "Causal decoder Transformer (decoder-only)                                                           1   \n",
       "Prefix decoder Transformer (decoder-only)                                                         NaN   \n",
       "Language modeling task (with Transformer archit...                                                  1   \n",
       "Cloze task (with Transformer architecture)                                                          0   \n",
       "Denoising autoencoding task (with Transformer a...                                                NaN   \n",
       "Mixture-of-Denoisers task                                                                         NaN   \n",
       "Dynamic batch size                                                                                  0   \n",
       "Learning rate decay                                                                                 1   \n",
       "Learning rate warmup                                                                                1   \n",
       "Inverse square root learning rate decay schedule                                                    0   \n",
       "Linear learning rate decay schedule                                                                 1   \n",
       "Cosine learning rate decay schedule                                                                 1   \n",
       "Adam optimizer                                                                                      0   \n",
       "AdamW optimizer                                                                                     1   \n",
       "Adafactor optimizer                                                                                 0   \n",
       "Dropout                                                                                             1   \n",
       "Weight decay                                                                                        1   \n",
       "Gradient clipping                                                                                   0   \n",
       "FlashAttention                                                                                      0   \n",
       "Mixed precision training                                                                            0   \n",
       "Instruction tuning                                                                                  0   \n",
       "RLHF                                                                                                0   \n",
       "PPO                                                                                                 0   \n",
       "A2C                                                                                               NaN   \n",
       "Prompting for in-context learning                                                                 NaN   \n",
       "Chain-of-thought                                                                                    0   \n",
       "\n",
       "                                                                                          Transformer  \n",
       "Algorithmic innovation                                                                                 \n",
       "Link                                                https://proceedings.neurips.cc/paper_files/pap...  \n",
       "Included                                                                                            0  \n",
       "Kaplan et al. scaling laws                                                                          0  \n",
       "Hoffmann et al. scaling laws                                                                        0  \n",
       "Transformer (general)                                                                               1  \n",
       "Sparse Attention                                                                                    0  \n",
       "Linear Attention                                                                                    0  \n",
       "Attention with locality-sensitive hashing                                                         NaN  \n",
       "Multi-Query Attention                                                                               0  \n",
       "Grouped Query Attention                                                                             0  \n",
       "RMSNorm                                                                                             0  \n",
       "LayerNorm                                                                                           1  \n",
       "Pre-normalization                                                                                   0  \n",
       "Learnable position embeddings                                                                       0  \n",
       "Sinusoidal position embeddings                                                                      1  \n",
       "Relative position embeddings                                                                        0  \n",
       "Rotary position embeddings                                                                          0  \n",
       "ALiBi positional encodings                                                                          0  \n",
       "ReLU activation                                                                                     1  \n",
       "GELU activation                                                                                     0  \n",
       "SwiGLU activation                                                                                   0  \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                                       0  \n",
       "Encoder-decoder Transformer                                                                       NaN  \n",
       "Causal decoder Transformer (decoder-only)                                                           0  \n",
       "Prefix decoder Transformer (decoder-only)                                                         NaN  \n",
       "Language modeling task (with Transformer archit...                                                  1  \n",
       "Cloze task (with Transformer architecture)                                                          0  \n",
       "Denoising autoencoding task (with Transformer a...                                                NaN  \n",
       "Mixture-of-Denoisers task                                                                         NaN  \n",
       "Dynamic batch size                                                                                  0  \n",
       "Learning rate decay                                                                                 1  \n",
       "Learning rate warmup                                                                                1  \n",
       "Inverse square root learning rate decay schedule                                                    1  \n",
       "Linear learning rate decay schedule                                                                 0  \n",
       "Cosine learning rate decay schedule                                                                 0  \n",
       "Adam optimizer                                                                                      1  \n",
       "AdamW optimizer                                                                                     0  \n",
       "Adafactor optimizer                                                                                 0  \n",
       "Dropout                                                                                             1  \n",
       "Weight decay                                                                                        0  \n",
       "Gradient clipping                                                                                   0  \n",
       "FlashAttention                                                                                      0  \n",
       "Mixed precision training                                                                            0  \n",
       "Instruction tuning                                                                                  0  \n",
       "RLHF                                                                                                0  \n",
       "PPO                                                                                                 0  \n",
       "A2C                                                                                               NaN  \n",
       "Prompting for in-context learning                                                                 NaN  \n",
       "Chain-of-thought                                                                                    0  \n",
       "\n",
       "[49 rows x 28 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace '?' values with 0\n",
    "occurrences_df = occurrences_df.replace('?', 0)\n",
    "occurrences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Algorithmic innovation',\n",
       " 'PaLM (540B)',\n",
       " 'Megatron-Turing NLG (530B)',\n",
       " 'ERNIE 3.0 Titan',\n",
       " 'Gopher (280B)',\n",
       " 'Chinchilla (70B)',\n",
       " 'PanGu-Σ',\n",
       " 'LLaMA (65B)',\n",
       " 'OPT-175B',\n",
       " 'Yuan 1.0',\n",
       " 'AlphaCode']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_systems = ['Algorithmic innovation'] + occurrences_df.columns[occurrences_df.iloc[1].astype(int).astype(bool)].tolist()\n",
    "keep_systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PaLM (540B)</th>\n",
       "      <th>Megatron-Turing NLG (530B)</th>\n",
       "      <th>ERNIE 3.0 Titan</th>\n",
       "      <th>Gopher (280B)</th>\n",
       "      <th>Chinchilla (70B)</th>\n",
       "      <th>PanGu-Σ</th>\n",
       "      <th>LLaMA (65B)</th>\n",
       "      <th>OPT-175B</th>\n",
       "      <th>Yuan 1.0</th>\n",
       "      <th>AlphaCode</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algorithmic innovation</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Link</th>\n",
       "      <td>https://arxiv.org/abs/2204.02311</td>\n",
       "      <td>https://arxiv.org/abs/2201.11990</td>\n",
       "      <td>https://arxiv.org/abs/2112.12731</td>\n",
       "      <td>https://arxiv.org/abs/2112.11446</td>\n",
       "      <td>https://arxiv.org/abs/2203.15556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://arxiv.org/abs/2302.13971</td>\n",
       "      <td>https://arxiv.org/abs/2205.01068</td>\n",
       "      <td>https://arxiv.org/abs/2110.04725</td>\n",
       "      <td>https://arxiv.org/abs/2203.07814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Included</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kaplan et al. scaling laws</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hoffmann et al. scaling laws</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transformer (general)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sparse Attention</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Attention</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention with locality-sensitive hashing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi-Query Attention</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grouped Query Attention</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LayerNorm</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pre-normalization</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Learnable position embeddings</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sinusoidal position embeddings</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Relative position embeddings</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rotary position embeddings</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALiBi positional encodings</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU activation</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GELU activation</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SwiGLU activation</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sparsely-Gated Mixture-of-Experts layer (MoE)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Encoder-decoder Transformer</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Causal decoder Transformer (decoder-only)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prefix decoder Transformer (decoder-only)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Language modeling task (with Transformer architecture)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cloze task (with Transformer architecture)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Denoising autoencoding task (with Transformer architecture)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mixture-of-Denoisers task</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dynamic batch size</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Learning rate decay</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Learning rate warmup</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inverse square root learning rate decay schedule</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear learning rate decay schedule</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cosine learning rate decay schedule</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adam optimizer</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdamW optimizer</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adafactor optimizer</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropout</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weight decay</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient clipping</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FlashAttention</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mixed precision training</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Instruction tuning</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RLHF</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PPO</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2C</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prompting for in-context learning</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chain-of-thought</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                         PaLM (540B)  \\\n",
       "Algorithmic innovation                                                                 \n",
       "Link                                                https://arxiv.org/abs/2204.02311   \n",
       "Included                                                                           1   \n",
       "Kaplan et al. scaling laws                                                         1   \n",
       "Hoffmann et al. scaling laws                                                       0   \n",
       "Transformer (general)                                                              1   \n",
       "Sparse Attention                                                                   0   \n",
       "Linear Attention                                                                   0   \n",
       "Attention with locality-sensitive hashing                                          0   \n",
       "Multi-Query Attention                                                              1   \n",
       "Grouped Query Attention                                                            0   \n",
       "RMSNorm                                                                            0   \n",
       "LayerNorm                                                                          1   \n",
       "Pre-normalization                                                                  1   \n",
       "Learnable position embeddings                                                      0   \n",
       "Sinusoidal position embeddings                                                     0   \n",
       "Relative position embeddings                                                       0   \n",
       "Rotary position embeddings                                                         1   \n",
       "ALiBi positional encodings                                                         0   \n",
       "ReLU activation                                                                    0   \n",
       "GELU activation                                                                    0   \n",
       "SwiGLU activation                                                                  1   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                      0   \n",
       "Encoder-decoder Transformer                                                        0   \n",
       "Causal decoder Transformer (decoder-only)                                          1   \n",
       "Prefix decoder Transformer (decoder-only)                                          0   \n",
       "Language modeling task (with Transformer archit...                                 1   \n",
       "Cloze task (with Transformer architecture)                                         0   \n",
       "Denoising autoencoding task (with Transformer a...                                 0   \n",
       "Mixture-of-Denoisers task                                                          0   \n",
       "Dynamic batch size                                                                 1   \n",
       "Learning rate decay                                                                1   \n",
       "Learning rate warmup                                                               0   \n",
       "Inverse square root learning rate decay schedule                                   1   \n",
       "Linear learning rate decay schedule                                                0   \n",
       "Cosine learning rate decay schedule                                                0   \n",
       "Adam optimizer                                                                     0   \n",
       "AdamW optimizer                                                                    0   \n",
       "Adafactor optimizer                                                                1   \n",
       "Dropout                                                                            0   \n",
       "Weight decay                                                                       1   \n",
       "Gradient clipping                                                                  1   \n",
       "FlashAttention                                                                     0   \n",
       "Mixed precision training                                                           0   \n",
       "Instruction tuning                                                                 1   \n",
       "RLHF                                                                               0   \n",
       "PPO                                                                                0   \n",
       "A2C                                                                                0   \n",
       "Prompting for in-context learning                                                  1   \n",
       "Chain-of-thought                                                                   1   \n",
       "\n",
       "                                                          Megatron-Turing NLG (530B)  \\\n",
       "Algorithmic innovation                                                                 \n",
       "Link                                                https://arxiv.org/abs/2201.11990   \n",
       "Included                                                                           1   \n",
       "Kaplan et al. scaling laws                                                         1   \n",
       "Hoffmann et al. scaling laws                                                       0   \n",
       "Transformer (general)                                                              1   \n",
       "Sparse Attention                                                                   0   \n",
       "Linear Attention                                                                   0   \n",
       "Attention with locality-sensitive hashing                                          0   \n",
       "Multi-Query Attention                                                              0   \n",
       "Grouped Query Attention                                                            0   \n",
       "RMSNorm                                                                            0   \n",
       "LayerNorm                                                                          1   \n",
       "Pre-normalization                                                                  1   \n",
       "Learnable position embeddings                                                      1   \n",
       "Sinusoidal position embeddings                                                     0   \n",
       "Relative position embeddings                                                       0   \n",
       "Rotary position embeddings                                                         0   \n",
       "ALiBi positional encodings                                                         0   \n",
       "ReLU activation                                                                    0   \n",
       "GELU activation                                                                    1   \n",
       "SwiGLU activation                                                                  0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                      0   \n",
       "Encoder-decoder Transformer                                                        0   \n",
       "Causal decoder Transformer (decoder-only)                                          1   \n",
       "Prefix decoder Transformer (decoder-only)                                          0   \n",
       "Language modeling task (with Transformer archit...                                 1   \n",
       "Cloze task (with Transformer architecture)                                         0   \n",
       "Denoising autoencoding task (with Transformer a...                                 0   \n",
       "Mixture-of-Denoisers task                                                          0   \n",
       "Dynamic batch size                                                                 1   \n",
       "Learning rate decay                                                                1   \n",
       "Learning rate warmup                                                               1   \n",
       "Inverse square root learning rate decay schedule                                   0   \n",
       "Linear learning rate decay schedule                                                0   \n",
       "Cosine learning rate decay schedule                                                1   \n",
       "Adam optimizer                                                                     1   \n",
       "AdamW optimizer                                                                    0   \n",
       "Adafactor optimizer                                                                0   \n",
       "Dropout                                                                            0   \n",
       "Weight decay                                                                       1   \n",
       "Gradient clipping                                                                  1   \n",
       "FlashAttention                                                                     0   \n",
       "Mixed precision training                                                           1   \n",
       "Instruction tuning                                                                 0   \n",
       "RLHF                                                                               0   \n",
       "PPO                                                                                0   \n",
       "A2C                                                                                0   \n",
       "Prompting for in-context learning                                                  1   \n",
       "Chain-of-thought                                                                   0   \n",
       "\n",
       "                                                                     ERNIE 3.0 Titan  \\\n",
       "Algorithmic innovation                                                                 \n",
       "Link                                                https://arxiv.org/abs/2112.12731   \n",
       "Included                                                                           1   \n",
       "Kaplan et al. scaling laws                                                         0   \n",
       "Hoffmann et al. scaling laws                                                       0   \n",
       "Transformer (general)                                                              1   \n",
       "Sparse Attention                                                                   0   \n",
       "Linear Attention                                                                   0   \n",
       "Attention with locality-sensitive hashing                                          0   \n",
       "Multi-Query Attention                                                              0   \n",
       "Grouped Query Attention                                                            0   \n",
       "RMSNorm                                                                            0   \n",
       "LayerNorm                                                                          1   \n",
       "Pre-normalization                                                                  0   \n",
       "Learnable position embeddings                                                      0   \n",
       "Sinusoidal position embeddings                                                     0   \n",
       "Relative position embeddings                                                       1   \n",
       "Rotary position embeddings                                                         0   \n",
       "ALiBi positional encodings                                                         0   \n",
       "ReLU activation                                                                    1   \n",
       "GELU activation                                                                    0   \n",
       "SwiGLU activation                                                                  0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                      0   \n",
       "Encoder-decoder Transformer                                                        1   \n",
       "Causal decoder Transformer (decoder-only)                                          0   \n",
       "Prefix decoder Transformer (decoder-only)                                          0   \n",
       "Language modeling task (with Transformer archit...                                 1   \n",
       "Cloze task (with Transformer architecture)                                         1   \n",
       "Denoising autoencoding task (with Transformer a...                                 0   \n",
       "Mixture-of-Denoisers task                                                          0   \n",
       "Dynamic batch size                                                                 0   \n",
       "Learning rate decay                                                                1   \n",
       "Learning rate warmup                                                               1   \n",
       "Inverse square root learning rate decay schedule                                   0   \n",
       "Linear learning rate decay schedule                                                1   \n",
       "Cosine learning rate decay schedule                                                0   \n",
       "Adam optimizer                                                                     1   \n",
       "AdamW optimizer                                                                    0   \n",
       "Adafactor optimizer                                                                0   \n",
       "Dropout                                                                            0   \n",
       "Weight decay                                                                       1   \n",
       "Gradient clipping                                                                  1   \n",
       "FlashAttention                                                                     0   \n",
       "Mixed precision training                                                           0   \n",
       "Instruction tuning                                                                 0   \n",
       "RLHF                                                                               0   \n",
       "PPO                                                                                0   \n",
       "A2C                                                                                0   \n",
       "Prompting for in-context learning                                                  1   \n",
       "Chain-of-thought                                                                   0   \n",
       "\n",
       "                                                                       Gopher (280B)  \\\n",
       "Algorithmic innovation                                                                 \n",
       "Link                                                https://arxiv.org/abs/2112.11446   \n",
       "Included                                                                           1   \n",
       "Kaplan et al. scaling laws                                                         1   \n",
       "Hoffmann et al. scaling laws                                                       0   \n",
       "Transformer (general)                                                              1   \n",
       "Sparse Attention                                                                   0   \n",
       "Linear Attention                                                                   0   \n",
       "Attention with locality-sensitive hashing                                          0   \n",
       "Multi-Query Attention                                                              0   \n",
       "Grouped Query Attention                                                            0   \n",
       "RMSNorm                                                                            1   \n",
       "LayerNorm                                                                          0   \n",
       "Pre-normalization                                                                  1   \n",
       "Learnable position embeddings                                                      0   \n",
       "Sinusoidal position embeddings                                                     0   \n",
       "Relative position embeddings                                                       1   \n",
       "Rotary position embeddings                                                         0   \n",
       "ALiBi positional encodings                                                         0   \n",
       "ReLU activation                                                                    0   \n",
       "GELU activation                                                                    1   \n",
       "SwiGLU activation                                                                  0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                      0   \n",
       "Encoder-decoder Transformer                                                        0   \n",
       "Causal decoder Transformer (decoder-only)                                          1   \n",
       "Prefix decoder Transformer (decoder-only)                                          0   \n",
       "Language modeling task (with Transformer archit...                                 1   \n",
       "Cloze task (with Transformer architecture)                                         0   \n",
       "Denoising autoencoding task (with Transformer a...                                 0   \n",
       "Mixture-of-Denoisers task                                                          0   \n",
       "Dynamic batch size                                                                 1   \n",
       "Learning rate decay                                                                1   \n",
       "Learning rate warmup                                                               1   \n",
       "Inverse square root learning rate decay schedule                                   0   \n",
       "Linear learning rate decay schedule                                                0   \n",
       "Cosine learning rate decay schedule                                                1   \n",
       "Adam optimizer                                                                     1   \n",
       "AdamW optimizer                                                                    0   \n",
       "Adafactor optimizer                                                                0   \n",
       "Dropout                                                                            1   \n",
       "Weight decay                                                                       1   \n",
       "Gradient clipping                                                                  1   \n",
       "FlashAttention                                                                     0   \n",
       "Mixed precision training                                                           0   \n",
       "Instruction tuning                                                                 0   \n",
       "RLHF                                                                               0   \n",
       "PPO                                                                                0   \n",
       "A2C                                                                                0   \n",
       "Prompting for in-context learning                                                  1   \n",
       "Chain-of-thought                                                                   0   \n",
       "\n",
       "                                                                    Chinchilla (70B)  \\\n",
       "Algorithmic innovation                                                                 \n",
       "Link                                                https://arxiv.org/abs/2203.15556   \n",
       "Included                                                                           1   \n",
       "Kaplan et al. scaling laws                                                         0   \n",
       "Hoffmann et al. scaling laws                                                       1   \n",
       "Transformer (general)                                                              1   \n",
       "Sparse Attention                                                                   0   \n",
       "Linear Attention                                                                   0   \n",
       "Attention with locality-sensitive hashing                                          0   \n",
       "Multi-Query Attention                                                              0   \n",
       "Grouped Query Attention                                                            0   \n",
       "RMSNorm                                                                            1   \n",
       "LayerNorm                                                                          0   \n",
       "Pre-normalization                                                                  1   \n",
       "Learnable position embeddings                                                      0   \n",
       "Sinusoidal position embeddings                                                     0   \n",
       "Relative position embeddings                                                       1   \n",
       "Rotary position embeddings                                                         0   \n",
       "ALiBi positional encodings                                                         0   \n",
       "ReLU activation                                                                    0   \n",
       "GELU activation                                                                    1   \n",
       "SwiGLU activation                                                                  0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                      0   \n",
       "Encoder-decoder Transformer                                                        0   \n",
       "Causal decoder Transformer (decoder-only)                                          1   \n",
       "Prefix decoder Transformer (decoder-only)                                          0   \n",
       "Language modeling task (with Transformer archit...                                 1   \n",
       "Cloze task (with Transformer architecture)                                         0   \n",
       "Denoising autoencoding task (with Transformer a...                                 0   \n",
       "Mixture-of-Denoisers task                                                          0   \n",
       "Dynamic batch size                                                                 1   \n",
       "Learning rate decay                                                                1   \n",
       "Learning rate warmup                                                               1   \n",
       "Inverse square root learning rate decay schedule                                   0   \n",
       "Linear learning rate decay schedule                                                0   \n",
       "Cosine learning rate decay schedule                                                1   \n",
       "Adam optimizer                                                                     0   \n",
       "AdamW optimizer                                                                    1   \n",
       "Adafactor optimizer                                                                0   \n",
       "Dropout                                                                            1   \n",
       "Weight decay                                                                       1   \n",
       "Gradient clipping                                                                  1   \n",
       "FlashAttention                                                                     0   \n",
       "Mixed precision training                                                           0   \n",
       "Instruction tuning                                                                 1   \n",
       "RLHF                                                                               1   \n",
       "PPO                                                                                0   \n",
       "A2C                                                                                1   \n",
       "Prompting for in-context learning                                                  1   \n",
       "Chain-of-thought                                                                   0   \n",
       "\n",
       "                                                   PanGu-Σ  \\\n",
       "Algorithmic innovation                                       \n",
       "Link                                                   NaN   \n",
       "Included                                                 1   \n",
       "Kaplan et al. scaling laws                               0   \n",
       "Hoffmann et al. scaling laws                             0   \n",
       "Transformer (general)                                    1   \n",
       "Sparse Attention                                         0   \n",
       "Linear Attention                                         0   \n",
       "Attention with locality-sensitive hashing                0   \n",
       "Multi-Query Attention                                    0   \n",
       "Grouped Query Attention                                  0   \n",
       "RMSNorm                                                  0   \n",
       "LayerNorm                                                1   \n",
       "Pre-normalization                                        1   \n",
       "Learnable position embeddings                            1   \n",
       "Sinusoidal position embeddings                           0   \n",
       "Relative position embeddings                             0   \n",
       "Rotary position embeddings                               0   \n",
       "ALiBi positional encodings                               0   \n",
       "ReLU activation                                          0   \n",
       "GELU activation                                          1   \n",
       "SwiGLU activation                                        0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)            1   \n",
       "Encoder-decoder Transformer                              0   \n",
       "Causal decoder Transformer (decoder-only)                1   \n",
       "Prefix decoder Transformer (decoder-only)                0   \n",
       "Language modeling task (with Transformer archit...       1   \n",
       "Cloze task (with Transformer architecture)               0   \n",
       "Denoising autoencoding task (with Transformer a...       0   \n",
       "Mixture-of-Denoisers task                                0   \n",
       "Dynamic batch size                                       0   \n",
       "Learning rate decay                                      1   \n",
       "Learning rate warmup                                     1   \n",
       "Inverse square root learning rate decay schedule         0   \n",
       "Linear learning rate decay schedule                      0   \n",
       "Cosine learning rate decay schedule                      0   \n",
       "Adam optimizer                                           1   \n",
       "AdamW optimizer                                          0   \n",
       "Adafactor optimizer                                      0   \n",
       "Dropout                                                  0   \n",
       "Weight decay                                             1   \n",
       "Gradient clipping                                        0   \n",
       "FlashAttention                                           0   \n",
       "Mixed precision training                                 1   \n",
       "Instruction tuning                                       0   \n",
       "RLHF                                                     0   \n",
       "PPO                                                      0   \n",
       "A2C                                                      0   \n",
       "Prompting for in-context learning                        1   \n",
       "Chain-of-thought                                         0   \n",
       "\n",
       "                                                                         LLaMA (65B)  \\\n",
       "Algorithmic innovation                                                                 \n",
       "Link                                                https://arxiv.org/abs/2302.13971   \n",
       "Included                                                                           1   \n",
       "Kaplan et al. scaling laws                                                         0   \n",
       "Hoffmann et al. scaling laws                                                       1   \n",
       "Transformer (general)                                                              1   \n",
       "Sparse Attention                                                                   0   \n",
       "Linear Attention                                                                   0   \n",
       "Attention with locality-sensitive hashing                                          0   \n",
       "Multi-Query Attention                                                              0   \n",
       "Grouped Query Attention                                                            0   \n",
       "RMSNorm                                                                            1   \n",
       "LayerNorm                                                                          0   \n",
       "Pre-normalization                                                                  1   \n",
       "Learnable position embeddings                                                      0   \n",
       "Sinusoidal position embeddings                                                     0   \n",
       "Relative position embeddings                                                       0   \n",
       "Rotary position embeddings                                                         1   \n",
       "ALiBi positional encodings                                                         0   \n",
       "ReLU activation                                                                    0   \n",
       "GELU activation                                                                    0   \n",
       "SwiGLU activation                                                                  1   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                      0   \n",
       "Encoder-decoder Transformer                                                        1   \n",
       "Causal decoder Transformer (decoder-only)                                          0   \n",
       "Prefix decoder Transformer (decoder-only)                                          0   \n",
       "Language modeling task (with Transformer archit...                                 1   \n",
       "Cloze task (with Transformer architecture)                                         0   \n",
       "Denoising autoencoding task (with Transformer a...                                 0   \n",
       "Mixture-of-Denoisers task                                                          0   \n",
       "Dynamic batch size                                                                 0   \n",
       "Learning rate decay                                                                1   \n",
       "Learning rate warmup                                                               1   \n",
       "Inverse square root learning rate decay schedule                                   0   \n",
       "Linear learning rate decay schedule                                                0   \n",
       "Cosine learning rate decay schedule                                                1   \n",
       "Adam optimizer                                                                     0   \n",
       "AdamW optimizer                                                                    1   \n",
       "Adafactor optimizer                                                                0   \n",
       "Dropout                                                                            0   \n",
       "Weight decay                                                                       1   \n",
       "Gradient clipping                                                                  1   \n",
       "FlashAttention                                                                     1   \n",
       "Mixed precision training                                                           0   \n",
       "Instruction tuning                                                                 1   \n",
       "RLHF                                                                               0   \n",
       "PPO                                                                                0   \n",
       "A2C                                                                                0   \n",
       "Prompting for in-context learning                                                  1   \n",
       "Chain-of-thought                                                                   0   \n",
       "\n",
       "                                                                            OPT-175B  \\\n",
       "Algorithmic innovation                                                                 \n",
       "Link                                                https://arxiv.org/abs/2205.01068   \n",
       "Included                                                                           1   \n",
       "Kaplan et al. scaling laws                                                         1   \n",
       "Hoffmann et al. scaling laws                                                       0   \n",
       "Transformer (general)                                                              1   \n",
       "Sparse Attention                                                                   1   \n",
       "Linear Attention                                                                   0   \n",
       "Attention with locality-sensitive hashing                                          0   \n",
       "Multi-Query Attention                                                              0   \n",
       "Grouped Query Attention                                                            0   \n",
       "RMSNorm                                                                            0   \n",
       "LayerNorm                                                                          1   \n",
       "Pre-normalization                                                                  1   \n",
       "Learnable position embeddings                                                      1   \n",
       "Sinusoidal position embeddings                                                     0   \n",
       "Relative position embeddings                                                       0   \n",
       "Rotary position embeddings                                                         0   \n",
       "ALiBi positional encodings                                                         0   \n",
       "ReLU activation                                                                    1   \n",
       "GELU activation                                                                    0   \n",
       "SwiGLU activation                                                                  0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                      0   \n",
       "Encoder-decoder Transformer                                                        0   \n",
       "Causal decoder Transformer (decoder-only)                                          1   \n",
       "Prefix decoder Transformer (decoder-only)                                          0   \n",
       "Language modeling task (with Transformer archit...                                 1   \n",
       "Cloze task (with Transformer architecture)                                         0   \n",
       "Denoising autoencoding task (with Transformer a...                                 0   \n",
       "Mixture-of-Denoisers task                                                          0   \n",
       "Dynamic batch size                                                                 0   \n",
       "Learning rate decay                                                                1   \n",
       "Learning rate warmup                                                               1   \n",
       "Inverse square root learning rate decay schedule                                   0   \n",
       "Linear learning rate decay schedule                                                1   \n",
       "Cosine learning rate decay schedule                                                0   \n",
       "Adam optimizer                                                                     0   \n",
       "AdamW optimizer                                                                    1   \n",
       "Adafactor optimizer                                                                0   \n",
       "Dropout                                                                            1   \n",
       "Weight decay                                                                       1   \n",
       "Gradient clipping                                                                  1   \n",
       "FlashAttention                                                                     0   \n",
       "Mixed precision training                                                           1   \n",
       "Instruction tuning                                                                 1   \n",
       "RLHF                                                                               0   \n",
       "PPO                                                                                0   \n",
       "A2C                                                                                0   \n",
       "Prompting for in-context learning                                                  1   \n",
       "Chain-of-thought                                                                   1   \n",
       "\n",
       "                                                                            Yuan 1.0  \\\n",
       "Algorithmic innovation                                                                 \n",
       "Link                                                https://arxiv.org/abs/2110.04725   \n",
       "Included                                                                           1   \n",
       "Kaplan et al. scaling laws                                                         0   \n",
       "Hoffmann et al. scaling laws                                                       0   \n",
       "Transformer (general)                                                              1   \n",
       "Sparse Attention                                                                   0   \n",
       "Linear Attention                                                                   0   \n",
       "Attention with locality-sensitive hashing                                          0   \n",
       "Multi-Query Attention                                                              0   \n",
       "Grouped Query Attention                                                            0   \n",
       "RMSNorm                                                                            0   \n",
       "LayerNorm                                                                          1   \n",
       "Pre-normalization                                                                  1   \n",
       "Learnable position embeddings                                                      1   \n",
       "Sinusoidal position embeddings                                                     0   \n",
       "Relative position embeddings                                                       0   \n",
       "Rotary position embeddings                                                         0   \n",
       "ALiBi positional encodings                                                         0   \n",
       "ReLU activation                                                                    0   \n",
       "GELU activation                                                                    1   \n",
       "SwiGLU activation                                                                  0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                      0   \n",
       "Encoder-decoder Transformer                                                        0   \n",
       "Causal decoder Transformer (decoder-only)                                          1   \n",
       "Prefix decoder Transformer (decoder-only)                                          0   \n",
       "Language modeling task (with Transformer archit...                                 1   \n",
       "Cloze task (with Transformer architecture)                                         0   \n",
       "Denoising autoencoding task (with Transformer a...                                 0   \n",
       "Mixture-of-Denoisers task                                                          0   \n",
       "Dynamic batch size                                                                 1   \n",
       "Learning rate decay                                                                1   \n",
       "Learning rate warmup                                                               1   \n",
       "Inverse square root learning rate decay schedule                                   0   \n",
       "Linear learning rate decay schedule                                                0   \n",
       "Cosine learning rate decay schedule                                                1   \n",
       "Adam optimizer                                                                     1   \n",
       "AdamW optimizer                                                                    0   \n",
       "Adafactor optimizer                                                                0   \n",
       "Dropout                                                                            1   \n",
       "Weight decay                                                                       1   \n",
       "Gradient clipping                                                                  1   \n",
       "FlashAttention                                                                     0   \n",
       "Mixed precision training                                                           0   \n",
       "Instruction tuning                                                                 0   \n",
       "RLHF                                                                               0   \n",
       "PPO                                                                                0   \n",
       "A2C                                                                                0   \n",
       "Prompting for in-context learning                                                  1   \n",
       "Chain-of-thought                                                                   0   \n",
       "\n",
       "                                                                           AlphaCode  \n",
       "Algorithmic innovation                                                                \n",
       "Link                                                https://arxiv.org/abs/2203.07814  \n",
       "Included                                                                           1  \n",
       "Kaplan et al. scaling laws                                                         0  \n",
       "Hoffmann et al. scaling laws                                                       0  \n",
       "Transformer (general)                                                              1  \n",
       "Sparse Attention                                                                   0  \n",
       "Linear Attention                                                                   0  \n",
       "Attention with locality-sensitive hashing                                          0  \n",
       "Multi-Query Attention                                                              1  \n",
       "Grouped Query Attention                                                            0  \n",
       "RMSNorm                                                                            0  \n",
       "LayerNorm                                                                          1  \n",
       "Pre-normalization                                                                  0  \n",
       "Learnable position embeddings                                                      0  \n",
       "Sinusoidal position embeddings                                                     1  \n",
       "Relative position embeddings                                                       0  \n",
       "Rotary position embeddings                                                         0  \n",
       "ALiBi positional encodings                                                         0  \n",
       "ReLU activation                                                                    1  \n",
       "GELU activation                                                                    0  \n",
       "SwiGLU activation                                                                  0  \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                                      0  \n",
       "Encoder-decoder Transformer                                                        1  \n",
       "Causal decoder Transformer (decoder-only)                                          0  \n",
       "Prefix decoder Transformer (decoder-only)                                          0  \n",
       "Language modeling task (with Transformer archit...                                 1  \n",
       "Cloze task (with Transformer architecture)                                         0  \n",
       "Denoising autoencoding task (with Transformer a...                                 1  \n",
       "Mixture-of-Denoisers task                                                          0  \n",
       "Dynamic batch size                                                                 0  \n",
       "Learning rate decay                                                                1  \n",
       "Learning rate warmup                                                               1  \n",
       "Inverse square root learning rate decay schedule                                   0  \n",
       "Linear learning rate decay schedule                                                0  \n",
       "Cosine learning rate decay schedule                                                1  \n",
       "Adam optimizer                                                                     0  \n",
       "AdamW optimizer                                                                    1  \n",
       "Adafactor optimizer                                                                0  \n",
       "Dropout                                                                            0  \n",
       "Weight decay                                                                       1  \n",
       "Gradient clipping                                                                  1  \n",
       "FlashAttention                                                                     0  \n",
       "Mixed precision training                                                           0  \n",
       "Instruction tuning                                                                 0  \n",
       "RLHF                                                                               0  \n",
       "PPO                                                                                0  \n",
       "A2C                                                                                0  \n",
       "Prompting for in-context learning                                                  0  \n",
       "Chain-of-thought                                                                   0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter columns\n",
    "selected_systems_occurrences_df = occurrences_df.filter(keep_systems)\n",
    "selected_systems_occurrences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PaLM (540B)</th>\n",
       "      <th>Megatron-Turing NLG (530B)</th>\n",
       "      <th>ERNIE 3.0 Titan</th>\n",
       "      <th>Gopher (280B)</th>\n",
       "      <th>Chinchilla (70B)</th>\n",
       "      <th>PanGu-Σ</th>\n",
       "      <th>LLaMA (65B)</th>\n",
       "      <th>OPT-175B</th>\n",
       "      <th>Yuan 1.0</th>\n",
       "      <th>AlphaCode</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Algorithmic innovation</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kaplan et al. scaling laws</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hoffmann et al. scaling laws</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transformer (general)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sparse Attention</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear Attention</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Attention with locality-sensitive hashing</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multi-Query Attention</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Grouped Query Attention</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RMSNorm</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LayerNorm</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pre-normalization</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Learnable position embeddings</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sinusoidal position embeddings</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Relative position embeddings</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rotary position embeddings</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ALiBi positional encodings</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU activation</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GELU activation</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SwiGLU activation</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sparsely-Gated Mixture-of-Experts layer (MoE)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Encoder-decoder Transformer</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Causal decoder Transformer (decoder-only)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prefix decoder Transformer (decoder-only)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Language modeling task (with Transformer architecture)</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cloze task (with Transformer architecture)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Denoising autoencoding task (with Transformer architecture)</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mixture-of-Denoisers task</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dynamic batch size</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Learning rate decay</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Learning rate warmup</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inverse square root learning rate decay schedule</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Linear learning rate decay schedule</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cosine learning rate decay schedule</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adam optimizer</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdamW optimizer</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adafactor optimizer</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropout</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weight decay</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient clipping</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FlashAttention</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mixed precision training</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Instruction tuning</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RLHF</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PPO</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2C</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prompting for in-context learning</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chain-of-thought</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   PaLM (540B)  \\\n",
       "Algorithmic innovation                                           \n",
       "Kaplan et al. scaling laws                                   1   \n",
       "Hoffmann et al. scaling laws                                 0   \n",
       "Transformer (general)                                        1   \n",
       "Sparse Attention                                             0   \n",
       "Linear Attention                                             0   \n",
       "Attention with locality-sensitive hashing                    0   \n",
       "Multi-Query Attention                                        1   \n",
       "Grouped Query Attention                                      0   \n",
       "RMSNorm                                                      0   \n",
       "LayerNorm                                                    1   \n",
       "Pre-normalization                                            1   \n",
       "Learnable position embeddings                                0   \n",
       "Sinusoidal position embeddings                               0   \n",
       "Relative position embeddings                                 0   \n",
       "Rotary position embeddings                                   1   \n",
       "ALiBi positional encodings                                   0   \n",
       "ReLU activation                                              0   \n",
       "GELU activation                                              0   \n",
       "SwiGLU activation                                            1   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                0   \n",
       "Encoder-decoder Transformer                                  0   \n",
       "Causal decoder Transformer (decoder-only)                    1   \n",
       "Prefix decoder Transformer (decoder-only)                    0   \n",
       "Language modeling task (with Transformer archit...           1   \n",
       "Cloze task (with Transformer architecture)                   0   \n",
       "Denoising autoencoding task (with Transformer a...           0   \n",
       "Mixture-of-Denoisers task                                    0   \n",
       "Dynamic batch size                                           1   \n",
       "Learning rate decay                                          1   \n",
       "Learning rate warmup                                         0   \n",
       "Inverse square root learning rate decay schedule             1   \n",
       "Linear learning rate decay schedule                          0   \n",
       "Cosine learning rate decay schedule                          0   \n",
       "Adam optimizer                                               0   \n",
       "AdamW optimizer                                              0   \n",
       "Adafactor optimizer                                          1   \n",
       "Dropout                                                      0   \n",
       "Weight decay                                                 1   \n",
       "Gradient clipping                                            1   \n",
       "FlashAttention                                               0   \n",
       "Mixed precision training                                     0   \n",
       "Instruction tuning                                           1   \n",
       "RLHF                                                         0   \n",
       "PPO                                                          0   \n",
       "A2C                                                          0   \n",
       "Prompting for in-context learning                            1   \n",
       "Chain-of-thought                                             1   \n",
       "\n",
       "                                                   Megatron-Turing NLG (530B)  \\\n",
       "Algorithmic innovation                                                          \n",
       "Kaplan et al. scaling laws                                                  1   \n",
       "Hoffmann et al. scaling laws                                                0   \n",
       "Transformer (general)                                                       1   \n",
       "Sparse Attention                                                            0   \n",
       "Linear Attention                                                            0   \n",
       "Attention with locality-sensitive hashing                                   0   \n",
       "Multi-Query Attention                                                       0   \n",
       "Grouped Query Attention                                                     0   \n",
       "RMSNorm                                                                     0   \n",
       "LayerNorm                                                                   1   \n",
       "Pre-normalization                                                           1   \n",
       "Learnable position embeddings                                               1   \n",
       "Sinusoidal position embeddings                                              0   \n",
       "Relative position embeddings                                                0   \n",
       "Rotary position embeddings                                                  0   \n",
       "ALiBi positional encodings                                                  0   \n",
       "ReLU activation                                                             0   \n",
       "GELU activation                                                             1   \n",
       "SwiGLU activation                                                           0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                               0   \n",
       "Encoder-decoder Transformer                                                 0   \n",
       "Causal decoder Transformer (decoder-only)                                   1   \n",
       "Prefix decoder Transformer (decoder-only)                                   0   \n",
       "Language modeling task (with Transformer archit...                          1   \n",
       "Cloze task (with Transformer architecture)                                  0   \n",
       "Denoising autoencoding task (with Transformer a...                          0   \n",
       "Mixture-of-Denoisers task                                                   0   \n",
       "Dynamic batch size                                                          1   \n",
       "Learning rate decay                                                         1   \n",
       "Learning rate warmup                                                        1   \n",
       "Inverse square root learning rate decay schedule                            0   \n",
       "Linear learning rate decay schedule                                         0   \n",
       "Cosine learning rate decay schedule                                         1   \n",
       "Adam optimizer                                                              1   \n",
       "AdamW optimizer                                                             0   \n",
       "Adafactor optimizer                                                         0   \n",
       "Dropout                                                                     0   \n",
       "Weight decay                                                                1   \n",
       "Gradient clipping                                                           1   \n",
       "FlashAttention                                                              0   \n",
       "Mixed precision training                                                    1   \n",
       "Instruction tuning                                                          0   \n",
       "RLHF                                                                        0   \n",
       "PPO                                                                         0   \n",
       "A2C                                                                         0   \n",
       "Prompting for in-context learning                                           1   \n",
       "Chain-of-thought                                                            0   \n",
       "\n",
       "                                                   ERNIE 3.0 Titan  \\\n",
       "Algorithmic innovation                                               \n",
       "Kaplan et al. scaling laws                                       0   \n",
       "Hoffmann et al. scaling laws                                     0   \n",
       "Transformer (general)                                            1   \n",
       "Sparse Attention                                                 0   \n",
       "Linear Attention                                                 0   \n",
       "Attention with locality-sensitive hashing                        0   \n",
       "Multi-Query Attention                                            0   \n",
       "Grouped Query Attention                                          0   \n",
       "RMSNorm                                                          0   \n",
       "LayerNorm                                                        1   \n",
       "Pre-normalization                                                0   \n",
       "Learnable position embeddings                                    0   \n",
       "Sinusoidal position embeddings                                   0   \n",
       "Relative position embeddings                                     1   \n",
       "Rotary position embeddings                                       0   \n",
       "ALiBi positional encodings                                       0   \n",
       "ReLU activation                                                  1   \n",
       "GELU activation                                                  0   \n",
       "SwiGLU activation                                                0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                    0   \n",
       "Encoder-decoder Transformer                                      1   \n",
       "Causal decoder Transformer (decoder-only)                        0   \n",
       "Prefix decoder Transformer (decoder-only)                        0   \n",
       "Language modeling task (with Transformer archit...               1   \n",
       "Cloze task (with Transformer architecture)                       1   \n",
       "Denoising autoencoding task (with Transformer a...               0   \n",
       "Mixture-of-Denoisers task                                        0   \n",
       "Dynamic batch size                                               0   \n",
       "Learning rate decay                                              1   \n",
       "Learning rate warmup                                             1   \n",
       "Inverse square root learning rate decay schedule                 0   \n",
       "Linear learning rate decay schedule                              1   \n",
       "Cosine learning rate decay schedule                              0   \n",
       "Adam optimizer                                                   1   \n",
       "AdamW optimizer                                                  0   \n",
       "Adafactor optimizer                                              0   \n",
       "Dropout                                                          0   \n",
       "Weight decay                                                     1   \n",
       "Gradient clipping                                                1   \n",
       "FlashAttention                                                   0   \n",
       "Mixed precision training                                         0   \n",
       "Instruction tuning                                               0   \n",
       "RLHF                                                             0   \n",
       "PPO                                                              0   \n",
       "A2C                                                              0   \n",
       "Prompting for in-context learning                                1   \n",
       "Chain-of-thought                                                 0   \n",
       "\n",
       "                                                   Gopher (280B)  \\\n",
       "Algorithmic innovation                                             \n",
       "Kaplan et al. scaling laws                                     1   \n",
       "Hoffmann et al. scaling laws                                   0   \n",
       "Transformer (general)                                          1   \n",
       "Sparse Attention                                               0   \n",
       "Linear Attention                                               0   \n",
       "Attention with locality-sensitive hashing                      0   \n",
       "Multi-Query Attention                                          0   \n",
       "Grouped Query Attention                                        0   \n",
       "RMSNorm                                                        1   \n",
       "LayerNorm                                                      0   \n",
       "Pre-normalization                                              1   \n",
       "Learnable position embeddings                                  0   \n",
       "Sinusoidal position embeddings                                 0   \n",
       "Relative position embeddings                                   1   \n",
       "Rotary position embeddings                                     0   \n",
       "ALiBi positional encodings                                     0   \n",
       "ReLU activation                                                0   \n",
       "GELU activation                                                1   \n",
       "SwiGLU activation                                              0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                  0   \n",
       "Encoder-decoder Transformer                                    0   \n",
       "Causal decoder Transformer (decoder-only)                      1   \n",
       "Prefix decoder Transformer (decoder-only)                      0   \n",
       "Language modeling task (with Transformer archit...             1   \n",
       "Cloze task (with Transformer architecture)                     0   \n",
       "Denoising autoencoding task (with Transformer a...             0   \n",
       "Mixture-of-Denoisers task                                      0   \n",
       "Dynamic batch size                                             1   \n",
       "Learning rate decay                                            1   \n",
       "Learning rate warmup                                           1   \n",
       "Inverse square root learning rate decay schedule               0   \n",
       "Linear learning rate decay schedule                            0   \n",
       "Cosine learning rate decay schedule                            1   \n",
       "Adam optimizer                                                 1   \n",
       "AdamW optimizer                                                0   \n",
       "Adafactor optimizer                                            0   \n",
       "Dropout                                                        1   \n",
       "Weight decay                                                   1   \n",
       "Gradient clipping                                              1   \n",
       "FlashAttention                                                 0   \n",
       "Mixed precision training                                       0   \n",
       "Instruction tuning                                             0   \n",
       "RLHF                                                           0   \n",
       "PPO                                                            0   \n",
       "A2C                                                            0   \n",
       "Prompting for in-context learning                              1   \n",
       "Chain-of-thought                                               0   \n",
       "\n",
       "                                                   Chinchilla (70B) PanGu-Σ  \\\n",
       "Algorithmic innovation                                                        \n",
       "Kaplan et al. scaling laws                                        0       0   \n",
       "Hoffmann et al. scaling laws                                      1       0   \n",
       "Transformer (general)                                             1       1   \n",
       "Sparse Attention                                                  0       0   \n",
       "Linear Attention                                                  0       0   \n",
       "Attention with locality-sensitive hashing                         0       0   \n",
       "Multi-Query Attention                                             0       0   \n",
       "Grouped Query Attention                                           0       0   \n",
       "RMSNorm                                                           1       0   \n",
       "LayerNorm                                                         0       1   \n",
       "Pre-normalization                                                 1       1   \n",
       "Learnable position embeddings                                     0       1   \n",
       "Sinusoidal position embeddings                                    0       0   \n",
       "Relative position embeddings                                      1       0   \n",
       "Rotary position embeddings                                        0       0   \n",
       "ALiBi positional encodings                                        0       0   \n",
       "ReLU activation                                                   0       0   \n",
       "GELU activation                                                   1       1   \n",
       "SwiGLU activation                                                 0       0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                     0       1   \n",
       "Encoder-decoder Transformer                                       0       0   \n",
       "Causal decoder Transformer (decoder-only)                         1       1   \n",
       "Prefix decoder Transformer (decoder-only)                         0       0   \n",
       "Language modeling task (with Transformer archit...                1       1   \n",
       "Cloze task (with Transformer architecture)                        0       0   \n",
       "Denoising autoencoding task (with Transformer a...                0       0   \n",
       "Mixture-of-Denoisers task                                         0       0   \n",
       "Dynamic batch size                                                1       0   \n",
       "Learning rate decay                                               1       1   \n",
       "Learning rate warmup                                              1       1   \n",
       "Inverse square root learning rate decay schedule                  0       0   \n",
       "Linear learning rate decay schedule                               0       0   \n",
       "Cosine learning rate decay schedule                               1       0   \n",
       "Adam optimizer                                                    0       1   \n",
       "AdamW optimizer                                                   1       0   \n",
       "Adafactor optimizer                                               0       0   \n",
       "Dropout                                                           1       0   \n",
       "Weight decay                                                      1       1   \n",
       "Gradient clipping                                                 1       0   \n",
       "FlashAttention                                                    0       0   \n",
       "Mixed precision training                                          0       1   \n",
       "Instruction tuning                                                1       0   \n",
       "RLHF                                                              1       0   \n",
       "PPO                                                               0       0   \n",
       "A2C                                                               1       0   \n",
       "Prompting for in-context learning                                 1       1   \n",
       "Chain-of-thought                                                  0       0   \n",
       "\n",
       "                                                   LLaMA (65B) OPT-175B  \\\n",
       "Algorithmic innovation                                                    \n",
       "Kaplan et al. scaling laws                                   0        1   \n",
       "Hoffmann et al. scaling laws                                 1        0   \n",
       "Transformer (general)                                        1        1   \n",
       "Sparse Attention                                             0        1   \n",
       "Linear Attention                                             0        0   \n",
       "Attention with locality-sensitive hashing                    0        0   \n",
       "Multi-Query Attention                                        0        0   \n",
       "Grouped Query Attention                                      0        0   \n",
       "RMSNorm                                                      1        0   \n",
       "LayerNorm                                                    0        1   \n",
       "Pre-normalization                                            1        1   \n",
       "Learnable position embeddings                                0        1   \n",
       "Sinusoidal position embeddings                               0        0   \n",
       "Relative position embeddings                                 0        0   \n",
       "Rotary position embeddings                                   1        0   \n",
       "ALiBi positional encodings                                   0        0   \n",
       "ReLU activation                                              0        1   \n",
       "GELU activation                                              0        0   \n",
       "SwiGLU activation                                            1        0   \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                0        0   \n",
       "Encoder-decoder Transformer                                  1        0   \n",
       "Causal decoder Transformer (decoder-only)                    0        1   \n",
       "Prefix decoder Transformer (decoder-only)                    0        0   \n",
       "Language modeling task (with Transformer archit...           1        1   \n",
       "Cloze task (with Transformer architecture)                   0        0   \n",
       "Denoising autoencoding task (with Transformer a...           0        0   \n",
       "Mixture-of-Denoisers task                                    0        0   \n",
       "Dynamic batch size                                           0        0   \n",
       "Learning rate decay                                          1        1   \n",
       "Learning rate warmup                                         1        1   \n",
       "Inverse square root learning rate decay schedule             0        0   \n",
       "Linear learning rate decay schedule                          0        1   \n",
       "Cosine learning rate decay schedule                          1        0   \n",
       "Adam optimizer                                               0        0   \n",
       "AdamW optimizer                                              1        1   \n",
       "Adafactor optimizer                                          0        0   \n",
       "Dropout                                                      0        1   \n",
       "Weight decay                                                 1        1   \n",
       "Gradient clipping                                            1        1   \n",
       "FlashAttention                                               1        0   \n",
       "Mixed precision training                                     0        1   \n",
       "Instruction tuning                                           1        1   \n",
       "RLHF                                                         0        0   \n",
       "PPO                                                          0        0   \n",
       "A2C                                                          0        0   \n",
       "Prompting for in-context learning                            1        1   \n",
       "Chain-of-thought                                             0        1   \n",
       "\n",
       "                                                   Yuan 1.0 AlphaCode  \n",
       "Algorithmic innovation                                                 \n",
       "Kaplan et al. scaling laws                                0         0  \n",
       "Hoffmann et al. scaling laws                              0         0  \n",
       "Transformer (general)                                     1         1  \n",
       "Sparse Attention                                          0         0  \n",
       "Linear Attention                                          0         0  \n",
       "Attention with locality-sensitive hashing                 0         0  \n",
       "Multi-Query Attention                                     0         1  \n",
       "Grouped Query Attention                                   0         0  \n",
       "RMSNorm                                                   0         0  \n",
       "LayerNorm                                                 1         1  \n",
       "Pre-normalization                                         1         0  \n",
       "Learnable position embeddings                             1         0  \n",
       "Sinusoidal position embeddings                            0         1  \n",
       "Relative position embeddings                              0         0  \n",
       "Rotary position embeddings                                0         0  \n",
       "ALiBi positional encodings                                0         0  \n",
       "ReLU activation                                           0         1  \n",
       "GELU activation                                           1         0  \n",
       "SwiGLU activation                                         0         0  \n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)             0         0  \n",
       "Encoder-decoder Transformer                               0         1  \n",
       "Causal decoder Transformer (decoder-only)                 1         0  \n",
       "Prefix decoder Transformer (decoder-only)                 0         0  \n",
       "Language modeling task (with Transformer archit...        1         1  \n",
       "Cloze task (with Transformer architecture)                0         0  \n",
       "Denoising autoencoding task (with Transformer a...        0         1  \n",
       "Mixture-of-Denoisers task                                 0         0  \n",
       "Dynamic batch size                                        1         0  \n",
       "Learning rate decay                                       1         1  \n",
       "Learning rate warmup                                      1         1  \n",
       "Inverse square root learning rate decay schedule          0         0  \n",
       "Linear learning rate decay schedule                       0         0  \n",
       "Cosine learning rate decay schedule                       1         1  \n",
       "Adam optimizer                                            1         0  \n",
       "AdamW optimizer                                           0         1  \n",
       "Adafactor optimizer                                       0         0  \n",
       "Dropout                                                   1         0  \n",
       "Weight decay                                              1         1  \n",
       "Gradient clipping                                         1         1  \n",
       "FlashAttention                                            0         0  \n",
       "Mixed precision training                                  0         0  \n",
       "Instruction tuning                                        0         0  \n",
       "RLHF                                                      0         0  \n",
       "PPO                                                       0         0  \n",
       "A2C                                                       0         0  \n",
       "Prompting for in-context learning                         1         0  \n",
       "Chain-of-thought                                          0         0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_systems_occurrences_df = selected_systems_occurrences_df.drop(['Link', 'Included'])\n",
    "selected_systems_occurrences_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make values integers\n",
    "selected_systems_occurrences_df = selected_systems_occurrences_df.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Algorithmic innovation\n",
       "Kaplan et al. scaling laws                                      4\n",
       "Hoffmann et al. scaling laws                                    2\n",
       "Transformer (general)                                          10\n",
       "Sparse Attention                                                1\n",
       "Linear Attention                                                0\n",
       "Attention with locality-sensitive hashing                       0\n",
       "Multi-Query Attention                                           2\n",
       "Grouped Query Attention                                         0\n",
       "RMSNorm                                                         3\n",
       "LayerNorm                                                       7\n",
       "Pre-normalization                                               8\n",
       "Learnable position embeddings                                   4\n",
       "Sinusoidal position embeddings                                  1\n",
       "Relative position embeddings                                    3\n",
       "Rotary position embeddings                                      2\n",
       "ALiBi positional encodings                                      0\n",
       "ReLU activation                                                 3\n",
       "GELU activation                                                 5\n",
       "SwiGLU activation                                               2\n",
       "Sparsely-Gated Mixture-of-Experts layer (MoE)                   1\n",
       "Encoder-decoder Transformer                                     3\n",
       "Causal decoder Transformer (decoder-only)                       7\n",
       "Prefix decoder Transformer (decoder-only)                       0\n",
       "Language modeling task (with Transformer architecture)         10\n",
       "Cloze task (with Transformer architecture)                      1\n",
       "Denoising autoencoding task (with Transformer architecture)     1\n",
       "Mixture-of-Denoisers task                                       0\n",
       "Dynamic batch size                                              5\n",
       "Learning rate decay                                            10\n",
       "Learning rate warmup                                            9\n",
       "Inverse square root learning rate decay schedule                1\n",
       "Linear learning rate decay schedule                             2\n",
       "Cosine learning rate decay schedule                             6\n",
       "Adam optimizer                                                  5\n",
       "AdamW optimizer                                                 4\n",
       "Adafactor optimizer                                             1\n",
       "Dropout                                                         4\n",
       "Weight decay                                                   10\n",
       "Gradient clipping                                               9\n",
       "FlashAttention                                                  1\n",
       "Mixed precision training                                        3\n",
       "Instruction tuning                                              4\n",
       "RLHF                                                            1\n",
       "PPO                                                             0\n",
       "A2C                                                             1\n",
       "Prompting for in-context learning                               9\n",
       "Chain-of-thought                                                2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum each row\n",
    "total_occurrences_by_innovation = selected_systems_occurrences_df.sum(axis=1)\n",
    "total_occurrences_by_innovation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaplan et al. scaling laws\n",
      "['Johns Hopkins University', 'OpenAI']\n",
      "None\n",
      "No alias for Johns Hopkins University\n",
      "OpenAI\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}})\n",
      "\n",
      "Hoffmann et al. scaling laws\n",
      "['Google DeepMind']\n",
      "DeepMind\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}})\n",
      "\n",
      "Transformer (general)\n",
      "['Google Brain', 'Google Research', 'University of Toronto']\n",
      "Google\n",
      "Google\n",
      "None\n",
      "No alias for University of Toronto\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}})\n",
      "\n",
      "Sparse Attention\n",
      "['OpenAI']\n",
      "OpenAI\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}})\n",
      "\n",
      "Multi-Query Attention\n",
      "['Google']\n",
      "Google\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}})\n",
      "\n",
      "RMSNorm\n",
      "['University of Edinburgh', 'University of Zurich']\n",
      "None\n",
      "No alias for University of Edinburgh\n",
      "None\n",
      "No alias for University of Zurich\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}})\n",
      "\n",
      "LayerNorm\n",
      "['University of Toronto', 'Google']\n",
      "None\n",
      "No alias for University of Toronto\n",
      "Google\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}})\n",
      "\n",
      "Pre-normalization\n",
      "['Facebook AI Research']\n",
      "Meta\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}})\n",
      "\n",
      "Learnable position embeddings\n",
      "['Facebook AI Research']\n",
      "Meta\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}})\n",
      "\n",
      "Sinusoidal position embeddings\n",
      "['Google Brain', 'Google Research', 'University of Toronto']\n",
      "Google\n",
      "Google\n",
      "None\n",
      "No alias for University of Toronto\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}})\n",
      "\n",
      "Relative position embeddings\n",
      "['Carnegie Mellon University', 'Google Brain']\n",
      "None\n",
      "No alias for Carnegie Mellon University\n",
      "Google\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}})\n",
      "\n",
      "Rotary position embeddings\n",
      "['Zhuiyi Technology Co., Ltd.']\n",
      "Zhuiyi\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}})\n",
      "\n",
      "ReLU activation\n",
      "['University of Toronto']\n",
      "None\n",
      "No alias for University of Toronto\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}})\n",
      "\n",
      "GELU activation\n",
      "['University of Chicago', 'Toyota Technological Institute at Chicago']\n",
      "None\n",
      "No alias for University of Chicago\n",
      "None\n",
      "No alias for Toyota Technological Institute at Chicago\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}})\n",
      "\n",
      "SwiGLU activation\n",
      "['Google']\n",
      "Google\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}})\n",
      "\n",
      "Sparsely-Gated Mixture-of-Experts layer (MoE)\n",
      "['Google Brain', 'Jagiellonian University']\n",
      "Google\n",
      "None\n",
      "No alias for Jagiellonian University\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}, 'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1}})\n",
      "\n",
      "Encoder-decoder Transformer\n",
      "['Google Brain', 'Google Research', 'University of Toronto']\n",
      "Google\n",
      "Google\n",
      "None\n",
      "No alias for University of Toronto\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}, 'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1}, 'Encoder-decoder Transformer': {'Google': 3}})\n",
      "\n",
      "Causal decoder Transformer (decoder-only)\n",
      "['Google Brain']\n",
      "Google\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}, 'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1}, 'Encoder-decoder Transformer': {'Google': 3}, 'Causal decoder Transformer (decoder-only)': {'Google': 7}})\n",
      "\n",
      "Language modeling task (with Transformer architecture)\n",
      "['Google Brain']\n",
      "Google\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}, 'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1}, 'Encoder-decoder Transformer': {'Google': 3}, 'Causal decoder Transformer (decoder-only)': {'Google': 7}, 'Language modeling task (with Transformer architecture)': {'Google': 10}})\n",
      "\n",
      "Cloze task (with Transformer architecture)\n",
      "['Google']\n",
      "Google\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}, 'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1}, 'Encoder-decoder Transformer': {'Google': 3}, 'Causal decoder Transformer (decoder-only)': {'Google': 7}, 'Language modeling task (with Transformer architecture)': {'Google': 10}, 'Cloze task (with Transformer architecture)': {'Google': 1}})\n",
      "\n",
      "Denoising autoencoding task (with Transformer architecture)\n",
      "['Facebook AI']\n",
      "Meta\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}, 'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1}, 'Encoder-decoder Transformer': {'Google': 3}, 'Causal decoder Transformer (decoder-only)': {'Google': 7}, 'Language modeling task (with Transformer architecture)': {'Google': 10}, 'Cloze task (with Transformer architecture)': {'Google': 1}, 'Denoising autoencoding task (with Transformer architecture)': {'Meta': 1}})\n",
      "\n",
      "Dynamic batch size\n",
      "['Google Brain']\n",
      "Google\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}, 'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1}, 'Encoder-decoder Transformer': {'Google': 3}, 'Causal decoder Transformer (decoder-only)': {'Google': 7}, 'Language modeling task (with Transformer architecture)': {'Google': 10}, 'Cloze task (with Transformer architecture)': {'Google': 1}, 'Denoising autoencoding task (with Transformer architecture)': {'Meta': 1}, 'Dynamic batch size': {'Google': 5}})\n",
      "\n",
      "Learning rate decay\n",
      "Learning rate warmup\n",
      "Inverse square root learning rate decay schedule\n",
      "Linear learning rate decay schedule\n",
      "Cosine learning rate decay schedule\n",
      "['University of Freiburg']\n",
      "None\n",
      "No alias for University of Freiburg\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}, 'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1}, 'Encoder-decoder Transformer': {'Google': 3}, 'Causal decoder Transformer (decoder-only)': {'Google': 7}, 'Language modeling task (with Transformer architecture)': {'Google': 10}, 'Cloze task (with Transformer architecture)': {'Google': 1}, 'Denoising autoencoding task (with Transformer architecture)': {'Meta': 1}, 'Dynamic batch size': {'Google': 5}})\n",
      "\n",
      "Adam optimizer\n",
      "['University of Amsterdam', 'University of Toronto']\n",
      "None\n",
      "No alias for University of Amsterdam\n",
      "None\n",
      "No alias for University of Toronto\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}, 'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1}, 'Encoder-decoder Transformer': {'Google': 3}, 'Causal decoder Transformer (decoder-only)': {'Google': 7}, 'Language modeling task (with Transformer architecture)': {'Google': 10}, 'Cloze task (with Transformer architecture)': {'Google': 1}, 'Denoising autoencoding task (with Transformer architecture)': {'Meta': 1}, 'Dynamic batch size': {'Google': 5}})\n",
      "\n",
      "AdamW optimizer\n",
      "['University of Freiburg']\n",
      "None\n",
      "No alias for University of Freiburg\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}, 'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1}, 'Encoder-decoder Transformer': {'Google': 3}, 'Causal decoder Transformer (decoder-only)': {'Google': 7}, 'Language modeling task (with Transformer architecture)': {'Google': 10}, 'Cloze task (with Transformer architecture)': {'Google': 1}, 'Denoising autoencoding task (with Transformer architecture)': {'Meta': 1}, 'Dynamic batch size': {'Google': 5}})\n",
      "\n",
      "Adafactor optimizer\n",
      "['Google Brain', 'UC Berkeley']\n",
      "Google\n",
      "None\n",
      "No alias for UC Berkeley\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}, 'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1}, 'Encoder-decoder Transformer': {'Google': 3}, 'Causal decoder Transformer (decoder-only)': {'Google': 7}, 'Language modeling task (with Transformer architecture)': {'Google': 10}, 'Cloze task (with Transformer architecture)': {'Google': 1}, 'Denoising autoencoding task (with Transformer architecture)': {'Meta': 1}, 'Dynamic batch size': {'Google': 5}, 'Adafactor optimizer': {'Google': 1}})\n",
      "\n",
      "Dropout\n",
      "['University of Toronto']\n",
      "None\n",
      "No alias for University of Toronto\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}, 'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1}, 'Encoder-decoder Transformer': {'Google': 3}, 'Causal decoder Transformer (decoder-only)': {'Google': 7}, 'Language modeling task (with Transformer architecture)': {'Google': 10}, 'Cloze task (with Transformer architecture)': {'Google': 1}, 'Denoising autoencoding task (with Transformer architecture)': {'Meta': 1}, 'Dynamic batch size': {'Google': 5}, 'Adafactor optimizer': {'Google': 1}})\n",
      "\n",
      "Weight decay\n",
      "Gradient clipping\n",
      "['University of Montreal', 'Brno University']\n",
      "None\n",
      "No alias for University of Montreal\n",
      "None\n",
      "No alias for Brno University\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}, 'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1}, 'Encoder-decoder Transformer': {'Google': 3}, 'Causal decoder Transformer (decoder-only)': {'Google': 7}, 'Language modeling task (with Transformer architecture)': {'Google': 10}, 'Cloze task (with Transformer architecture)': {'Google': 1}, 'Denoising autoencoding task (with Transformer architecture)': {'Meta': 1}, 'Dynamic batch size': {'Google': 5}, 'Adafactor optimizer': {'Google': 1}})\n",
      "\n",
      "FlashAttention\n",
      "['Stanford University', 'University at Buffalo, SUNY']\n",
      "None\n",
      "No alias for Stanford University\n",
      "None\n",
      "No alias for University at Buffalo, SUNY\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}, 'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1}, 'Encoder-decoder Transformer': {'Google': 3}, 'Causal decoder Transformer (decoder-only)': {'Google': 7}, 'Language modeling task (with Transformer architecture)': {'Google': 10}, 'Cloze task (with Transformer architecture)': {'Google': 1}, 'Denoising autoencoding task (with Transformer architecture)': {'Meta': 1}, 'Dynamic batch size': {'Google': 5}, 'Adafactor optimizer': {'Google': 1}})\n",
      "\n",
      "Mixed precision training\n",
      "['Baidu Research', 'NVIDIA']\n",
      "Baidu\n",
      "NVIDIA\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}, 'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1}, 'Encoder-decoder Transformer': {'Google': 3}, 'Causal decoder Transformer (decoder-only)': {'Google': 7}, 'Language modeling task (with Transformer architecture)': {'Google': 10}, 'Cloze task (with Transformer architecture)': {'Google': 1}, 'Denoising autoencoding task (with Transformer architecture)': {'Meta': 1}, 'Dynamic batch size': {'Google': 5}, 'Adafactor optimizer': {'Google': 1}, 'Mixed precision training': {'Baidu': 3, 'NVIDIA': 3}})\n",
      "\n",
      "Instruction tuning\n",
      "['OpenAI']\n",
      "OpenAI\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}, 'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1}, 'Encoder-decoder Transformer': {'Google': 3}, 'Causal decoder Transformer (decoder-only)': {'Google': 7}, 'Language modeling task (with Transformer architecture)': {'Google': 10}, 'Cloze task (with Transformer architecture)': {'Google': 1}, 'Denoising autoencoding task (with Transformer architecture)': {'Meta': 1}, 'Dynamic batch size': {'Google': 5}, 'Adafactor optimizer': {'Google': 1}, 'Mixed precision training': {'Baidu': 3, 'NVIDIA': 3}, 'Instruction tuning': {'OpenAI': 4}})\n",
      "\n",
      "RLHF\n",
      "['OpenAI', 'DeepMind']\n",
      "OpenAI\n",
      "DeepMind\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}, 'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1}, 'Encoder-decoder Transformer': {'Google': 3}, 'Causal decoder Transformer (decoder-only)': {'Google': 7}, 'Language modeling task (with Transformer architecture)': {'Google': 10}, 'Cloze task (with Transformer architecture)': {'Google': 1}, 'Denoising autoencoding task (with Transformer architecture)': {'Meta': 1}, 'Dynamic batch size': {'Google': 5}, 'Adafactor optimizer': {'Google': 1}, 'Mixed precision training': {'Baidu': 3, 'NVIDIA': 3}, 'Instruction tuning': {'OpenAI': 4}, 'RLHF': {'OpenAI': 1, 'DeepMind': 1}})\n",
      "\n",
      "A2C\n",
      "['Google DeepMind', 'University of Montreal']\n",
      "DeepMind\n",
      "None\n",
      "No alias for University of Montreal\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}, 'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1}, 'Encoder-decoder Transformer': {'Google': 3}, 'Causal decoder Transformer (decoder-only)': {'Google': 7}, 'Language modeling task (with Transformer architecture)': {'Google': 10}, 'Cloze task (with Transformer architecture)': {'Google': 1}, 'Denoising autoencoding task (with Transformer architecture)': {'Meta': 1}, 'Dynamic batch size': {'Google': 5}, 'Adafactor optimizer': {'Google': 1}, 'Mixed precision training': {'Baidu': 3, 'NVIDIA': 3}, 'Instruction tuning': {'OpenAI': 4}, 'RLHF': {'OpenAI': 1, 'DeepMind': 1}, 'A2C': {'DeepMind': 1}})\n",
      "\n",
      "Prompting for in-context learning\n",
      "['OpenAI']\n",
      "OpenAI\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}, 'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1}, 'Encoder-decoder Transformer': {'Google': 3}, 'Causal decoder Transformer (decoder-only)': {'Google': 7}, 'Language modeling task (with Transformer architecture)': {'Google': 10}, 'Cloze task (with Transformer architecture)': {'Google': 1}, 'Denoising autoencoding task (with Transformer architecture)': {'Meta': 1}, 'Dynamic batch size': {'Google': 5}, 'Adafactor optimizer': {'Google': 1}, 'Mixed precision training': {'Baidu': 3, 'NVIDIA': 3}, 'Instruction tuning': {'OpenAI': 4}, 'RLHF': {'OpenAI': 1, 'DeepMind': 1}, 'A2C': {'DeepMind': 1}, 'Prompting for in-context learning': {'OpenAI': 9}})\n",
      "\n",
      "Chain-of-thought\n",
      "['Google Brain']\n",
      "Google\n",
      "defaultdict(<class 'dict'>, {'Kaplan et al. scaling laws': {'OpenAI': 4}, 'Hoffmann et al. scaling laws': {'DeepMind': 2}, 'Transformer (general)': {'Google': 10}, 'Sparse Attention': {'OpenAI': 1}, 'Multi-Query Attention': {'Google': 2}, 'LayerNorm': {'Google': 7}, 'Pre-normalization': {'Meta': 8}, 'Learnable position embeddings': {'Meta': 4}, 'Sinusoidal position embeddings': {'Google': 1}, 'Relative position embeddings': {'Google': 3}, 'Rotary position embeddings': {'Zhuiyi': 2}, 'SwiGLU activation': {'Google': 2}, 'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1}, 'Encoder-decoder Transformer': {'Google': 3}, 'Causal decoder Transformer (decoder-only)': {'Google': 7}, 'Language modeling task (with Transformer architecture)': {'Google': 10}, 'Cloze task (with Transformer architecture)': {'Google': 1}, 'Denoising autoencoding task (with Transformer architecture)': {'Meta': 1}, 'Dynamic batch size': {'Google': 5}, 'Adafactor optimizer': {'Google': 1}, 'Mixed precision training': {'Baidu': 3, 'NVIDIA': 3}, 'Instruction tuning': {'OpenAI': 4}, 'RLHF': {'OpenAI': 1, 'DeepMind': 1}, 'A2C': {'DeepMind': 1}, 'Prompting for in-context learning': {'OpenAI': 9}, 'Chain-of-thought': {'Google': 2}})\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'Kaplan et al. scaling laws': {'OpenAI': 4},\n",
       "             'Hoffmann et al. scaling laws': {'DeepMind': 2},\n",
       "             'Transformer (general)': {'Google': 10},\n",
       "             'Sparse Attention': {'OpenAI': 1},\n",
       "             'Multi-Query Attention': {'Google': 2},\n",
       "             'LayerNorm': {'Google': 7},\n",
       "             'Pre-normalization': {'Meta': 8},\n",
       "             'Learnable position embeddings': {'Meta': 4},\n",
       "             'Sinusoidal position embeddings': {'Google': 1},\n",
       "             'Relative position embeddings': {'Google': 3},\n",
       "             'Rotary position embeddings': {'Zhuiyi': 2},\n",
       "             'SwiGLU activation': {'Google': 2},\n",
       "             'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1},\n",
       "             'Encoder-decoder Transformer': {'Google': 3},\n",
       "             'Causal decoder Transformer (decoder-only)': {'Google': 7},\n",
       "             'Language modeling task (with Transformer architecture)': {'Google': 10},\n",
       "             'Cloze task (with Transformer architecture)': {'Google': 1},\n",
       "             'Denoising autoencoding task (with Transformer architecture)': {'Meta': 1},\n",
       "             'Dynamic batch size': {'Google': 5},\n",
       "             'Adafactor optimizer': {'Google': 1},\n",
       "             'Mixed precision training': {'Baidu': 3, 'NVIDIA': 3},\n",
       "             'Instruction tuning': {'OpenAI': 4},\n",
       "             'RLHF': {'OpenAI': 1, 'DeepMind': 1},\n",
       "             'A2C': {'DeepMind': 1},\n",
       "             'Prompting for in-context learning': {'OpenAI': 9},\n",
       "             'Chain-of-thought': {'Google': 2}})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "institution_key_algorithm_occurrences = defaultdict(dict)\n",
    "for innovation, occurrence_count in total_occurrences_by_innovation.items():\n",
    "    if occurrence_count == 0:\n",
    "        continue\n",
    "    print(innovation)\n",
    "    matching_origin = origins_df[origins_df['Algorithmic innovation'] == innovation]\n",
    "    if len(matching_origin) == 0:\n",
    "        continue\n",
    "    origin_row_number = matching_origin.index[0]\n",
    "    origin_affiliations = origins_df.loc[origin_row_number]['Origin affiliations']\n",
    "    origin_affiliations = [affiliation.strip() for affiliation in origin_affiliations.split(';')]\n",
    "    print(origin_affiliations)\n",
    "    for affiliation in origin_affiliations:\n",
    "        alias = institution_aliases.get(affiliation)\n",
    "        print(alias)\n",
    "        if alias is not None:\n",
    "            institution_key_algorithm_occurrences[innovation][alias] = occurrence_count\n",
    "        else:\n",
    "            print(f'No alias for {affiliation}')\n",
    "    print(institution_key_algorithm_occurrences)\n",
    "    print()\n",
    "\n",
    "institution_key_algorithm_occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Kaplan et al. scaling laws\n",
      "2 Hoffmann et al. scaling laws\n",
      "3 Transformer (general)\n",
      "4 Sparse Attention\n",
      "5 Multi-Query Attention\n",
      "6 LayerNorm\n",
      "7 Pre-normalization\n",
      "8 Learnable position embeddings\n",
      "9 Sinusoidal position embeddings\n",
      "10 Relative position embeddings\n",
      "11 Rotary position embeddings\n",
      "12 SwiGLU activation\n",
      "13 Sparsely-Gated Mixture-of-Experts layer (MoE)\n",
      "14 Encoder-decoder Transformer\n",
      "15 Causal decoder Transformer (decoder-only)\n",
      "16 Language modeling task (with Transformer architecture)\n",
      "17 Cloze task (with Transformer architecture)\n",
      "18 Denoising autoencoding task (with Transformer architecture)\n",
      "19 Dynamic batch size\n",
      "20 Adafactor optimizer\n",
      "21 Mixed precision training\n",
      "22 Instruction tuning\n",
      "23 RLHF\n",
      "24 A2C\n",
      "25 Prompting for in-context learning\n",
      "26 Chain-of-thought\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for innovation, occurrence_count in total_occurrences_by_innovation.items():\n",
    "    if innovation in institution_key_algorithm_occurrences:\n",
    "        i += 1\n",
    "        print(i, innovation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Sparse Attention",
         "type": "bar",
         "x": [
          "OpenAI"
         ],
         "y": [
          1
         ]
        },
        {
         "name": "Sinusoidal position embeddings",
         "type": "bar",
         "x": [
          "Google"
         ],
         "y": [
          1
         ]
        },
        {
         "name": "Sparsely-Gated Mixture-of-Experts layer (MoE)",
         "type": "bar",
         "x": [
          "Google"
         ],
         "y": [
          1
         ]
        },
        {
         "name": "Cloze task (with Transformer architecture)",
         "type": "bar",
         "x": [
          "Google"
         ],
         "y": [
          1
         ]
        },
        {
         "name": "Denoising autoencoding task (with Transformer architecture)",
         "type": "bar",
         "x": [
          "Meta"
         ],
         "y": [
          1
         ]
        },
        {
         "name": "Adafactor optimizer",
         "type": "bar",
         "x": [
          "Google"
         ],
         "y": [
          1
         ]
        },
        {
         "name": "RLHF",
         "type": "bar",
         "x": [
          "OpenAI",
          "DeepMind"
         ],
         "y": [
          1,
          1
         ]
        },
        {
         "name": "A2C",
         "type": "bar",
         "x": [
          "DeepMind"
         ],
         "y": [
          1
         ]
        },
        {
         "name": "Hoffmann et al. scaling laws",
         "type": "bar",
         "x": [
          "DeepMind"
         ],
         "y": [
          2
         ]
        },
        {
         "name": "Multi-Query Attention",
         "type": "bar",
         "x": [
          "Google"
         ],
         "y": [
          2
         ]
        },
        {
         "name": "Rotary position embeddings",
         "type": "bar",
         "x": [
          "Zhuiyi"
         ],
         "y": [
          2
         ]
        },
        {
         "name": "SwiGLU activation",
         "type": "bar",
         "x": [
          "Google"
         ],
         "y": [
          2
         ]
        },
        {
         "name": "Chain-of-thought",
         "type": "bar",
         "x": [
          "Google"
         ],
         "y": [
          2
         ]
        },
        {
         "name": "Relative position embeddings",
         "type": "bar",
         "x": [
          "Google"
         ],
         "y": [
          3
         ]
        },
        {
         "name": "Encoder-decoder Transformer",
         "type": "bar",
         "x": [
          "Google"
         ],
         "y": [
          3
         ]
        },
        {
         "name": "Mixed precision training",
         "type": "bar",
         "x": [
          "Baidu",
          "NVIDIA"
         ],
         "y": [
          3,
          3
         ]
        },
        {
         "name": "Kaplan et al. scaling laws",
         "type": "bar",
         "x": [
          "OpenAI"
         ],
         "y": [
          4
         ]
        },
        {
         "name": "Learnable position embeddings",
         "type": "bar",
         "x": [
          "Meta"
         ],
         "y": [
          4
         ]
        },
        {
         "name": "Instruction tuning",
         "type": "bar",
         "x": [
          "OpenAI"
         ],
         "y": [
          4
         ]
        },
        {
         "name": "Dynamic batch size",
         "type": "bar",
         "x": [
          "Google"
         ],
         "y": [
          5
         ]
        },
        {
         "name": "LayerNorm",
         "type": "bar",
         "x": [
          "Google"
         ],
         "y": [
          7
         ]
        },
        {
         "name": "Causal decoder Transformer (decoder-only)",
         "type": "bar",
         "x": [
          "Google"
         ],
         "y": [
          7
         ]
        },
        {
         "name": "Pre-normalization",
         "type": "bar",
         "x": [
          "Meta"
         ],
         "y": [
          8
         ]
        },
        {
         "name": "Prompting for in-context learning",
         "type": "bar",
         "x": [
          "OpenAI"
         ],
         "y": [
          9
         ]
        },
        {
         "name": "Transformer (general)",
         "type": "bar",
         "x": [
          "Google"
         ],
         "y": [
          10
         ]
        },
        {
         "name": "Language modeling task (with Transformer architecture)",
         "type": "bar",
         "x": [
          "Google"
         ],
         "y": [
          10
         ]
        }
       ],
       "layout": {
        "annotations": [
         {
          "arrowcolor": "#1F95BD",
          "ax": 35,
          "ay": -70,
          "font": {
           "size": 10
          },
          "showarrow": true,
          "text": "Sparse attention",
          "x": "OpenAI",
          "xanchor": "left",
          "y": 0.5
         },
         {
          "arrowcolor": "#1F95BD",
          "ax": 25,
          "ay": -25,
          "font": {
           "size": 10
          },
          "showarrow": true,
          "text": "Chinchilla scaling laws",
          "x": "DeepMind",
          "xanchor": "left",
          "y": 3
         },
         {
          "arrowcolor": "#1F95BD",
          "ax": 35,
          "ay": -90,
          "font": {
           "size": 10
          },
          "showarrow": true,
          "text": "SwiGLU activation",
          "x": "Google",
          "xanchor": "left",
          "y": 7
         },
         {
          "arrowcolor": "#1F95BD",
          "ax": 15,
          "ay": -15,
          "font": {
           "size": 10
          },
          "showarrow": true,
          "text": "Mixed-precision training",
          "x": "NVIDIA",
          "xanchor": "left",
          "y": 1.5
         },
         {
          "arrowcolor": "#1F95BD",
          "ax": 35,
          "ay": -65,
          "font": {
           "size": 10
          },
          "showarrow": true,
          "text": "Instruction tuning",
          "x": "OpenAI",
          "xanchor": "left",
          "y": 8
         },
         {
          "arrowcolor": "#1F95BD",
          "ax": 35,
          "ay": -60,
          "font": {
           "size": 10
          },
          "showarrow": true,
          "text": "LayerNorm",
          "x": "Google",
          "xanchor": "left",
          "y": 24.5
         },
         {
          "arrowcolor": "#1F95BD",
          "ax": 25,
          "ay": -25,
          "font": {
           "size": 10
          },
          "showarrow": true,
          "text": "Pre-normalization",
          "x": "Meta",
          "xanchor": "left",
          "y": 9
         },
         {
          "arrowcolor": "#1F95BD",
          "ax": 35,
          "ay": -35,
          "font": {
           "size": 10
          },
          "showarrow": true,
          "text": "Transformer",
          "x": "Google",
          "xanchor": "left",
          "y": 40
         },
         {
          "font": {
           "size": 12
          },
          "showarrow": false,
          "text": "Adoption frequency in 10 largest LMs",
          "x": -0.06,
          "xref": "x domain",
          "y": 1.15,
          "yref": "y domain"
         }
        ],
        "autosize": false,
        "barmode": "stack",
        "font": {
         "size": 10
        },
        "height": 250,
        "margin": {
         "b": 20,
         "l": 20,
         "r": 20,
         "t": 40
        },
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "color": "#034752",
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "marker": {
             "color": "#034752"
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "bargap": 0.3,
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f",
           "family": "Messina Sans"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "font": {
            "color": "#09323A",
            "family": "Messina Serif",
            "size": 16
           },
           "x": 0
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#5C737B",
           "linewidth": 1,
           "tickcolor": "#5C737B",
           "tickfont": {
            "color": "#435359",
            "size": 10
           },
           "ticks": "",
           "title": {
            "font": {
             "size": 12
            },
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#5C737B",
           "linewidth": 1,
           "tickcolor": "#5C737B",
           "tickfont": {
            "color": "#435359",
            "size": 10
           },
           "ticks": "",
           "title": {
            "font": {
             "size": 12
            },
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "x": 0.5
        },
        "width": 480,
        "xaxis": {
         "categoryorder": "total descending",
         "showline": true,
         "ticklen": 5,
         "ticks": "outside",
         "title": {
          "text": "Company responsible for innovation"
         }
        },
        "yaxis": {
         "range": [
          0,
          59
         ],
         "title": {
          "standoff": 0,
          "text": ""
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate the stacked bar chart\n",
    "fig = go.Figure()\n",
    "\n",
    "annotations = []\n",
    "company_count_stack = defaultdict(int)\n",
    "\n",
    "# Add bar traces\n",
    "for innovation, company_counts in sorted(institution_key_algorithm_occurrences.items(), key=lambda x: min(x[1].values())):\n",
    "    y_values = list(company_counts.values())\n",
    "    x_values = list(company_counts.keys())\n",
    "    fig.add_trace(go.Bar(name=innovation, x=x_values, y=y_values))\n",
    "\n",
    "    # Add annotations\n",
    "    for x, y in zip(x_values, y_values):\n",
    "        start_y = company_count_stack[x]\n",
    "        end_y = start_y + y\n",
    "        middle_y = (start_y + end_y) / 2\n",
    "        company_count_stack[x] = end_y\n",
    "        annotation_font = dict(size=10)\n",
    "        arrowcolor = '#1F95BD'\n",
    "        if 'Transformer (general)' in innovation:\n",
    "            annotations.append(dict(x=x, y=middle_y, xanchor='left', ax=35, ay=-35, showarrow=True, text='Transformer', font=annotation_font, arrowcolor=arrowcolor))\n",
    "        elif 'LayerNorm' in innovation:\n",
    "            annotations.append(dict(x=x, y=middle_y, xanchor='left', ax=35, ay=-60, showarrow=True, text='LayerNorm', font=annotation_font, arrowcolor=arrowcolor))\n",
    "        elif 'SwiGLU' in innovation:\n",
    "            annotations.append(dict(x=x, y=middle_y, xanchor='left', ax=35, ay=-90, showarrow=True, text='SwiGLU activation', font=annotation_font, arrowcolor=arrowcolor))\n",
    "        # elif 'In-context' in innovation:\n",
    "        #     annotations.append(dict(x=x, y=middle_y, xanchor='left', ax=35, ay=-45, showarrow=True, text='Prompting for in-context learning', font=annotation_font, arrowcolor=arrowcolor))\n",
    "        elif 'Instruction' in innovation:\n",
    "            annotations.append(dict(x=x, y=middle_y, xanchor='left', ax=35, ay=-65, showarrow=True, text=innovation, font=annotation_font, arrowcolor=arrowcolor))\n",
    "        elif 'Sparse Attention' in innovation:\n",
    "            annotations.append(dict(x=x, y=middle_y, xanchor='left', ax=35, ay=-70, showarrow=True, text='Sparse attention', font=annotation_font, arrowcolor=arrowcolor))\n",
    "        elif 'Pre-' in innovation:\n",
    "            annotations.append(dict(x=x, y=middle_y, xanchor='left', ax=25, ay=-25, showarrow=True, text=innovation, font=annotation_font, arrowcolor=arrowcolor))\n",
    "        elif 'Hoff' in innovation:\n",
    "            annotations.append(dict(x=x, y=middle_y, xanchor='left', ax=25, ay=-25, showarrow=True, text='Chinchilla scaling laws', font=annotation_font, arrowcolor=arrowcolor))\n",
    "        elif 'Mixed' in innovation and x == 'NVIDIA':\n",
    "            annotations.append(dict(x=x, y=middle_y, xanchor='left', ax=15, ay=-15, showarrow=True, text='Mixed-precision training', font=annotation_font, arrowcolor=arrowcolor))\n",
    "\n",
    "\n",
    "## Plot layout\n",
    "fig.update_layout(\n",
    "    barmode='stack',\n",
    "    xaxis={'categoryorder':'total descending'},\n",
    "    # title='Occurrence of innovations in the top 10 largest LMs',\n",
    "    xaxis_title='Company responsible for innovation',\n",
    "    yaxis_title='Adoption frequency in 10 largest LMs',\n",
    "    showlegend=False,\n",
    "    annotations=annotations,\n",
    "    autosize=False,\n",
    "    width=480,\n",
    "    height=250,\n",
    "    # height=360,\n",
    "    title_x=0.5,\n",
    "    font=dict(size=10),\n",
    "    margin=dict(l=20, r=20, t=30, b=20),\n",
    ")\n",
    "\n",
    "plotting.prettify_bar_chart(fig, rotate_x_labels=False)\n",
    "fig.update_layout(margin=dict(t=40))\n",
    "fig.update_yaxes(range=[0, 59])\n",
    "\n",
    "## Save plot\n",
    "save_plot(fig, result_file_location, 'key_innovations_occurrence')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'OpenAI': {'Kaplan et al. scaling laws': 4,\n",
       "              'Sparse Attention': 1,\n",
       "              'Instruction tuning': 4,\n",
       "              'RLHF': 1,\n",
       "              'Prompting for in-context learning': 9},\n",
       "             'DeepMind': {'Hoffmann et al. scaling laws': 2,\n",
       "              'RLHF': 1,\n",
       "              'A2C': 1},\n",
       "             'Google': {'Transformer (general)': 10,\n",
       "              'Multi-Query Attention': 2,\n",
       "              'LayerNorm': 7,\n",
       "              'Sinusoidal position embeddings': 1,\n",
       "              'Relative position embeddings': 3,\n",
       "              'SwiGLU activation': 2,\n",
       "              'Sparsely-Gated Mixture-of-Experts layer (MoE)': 1,\n",
       "              'Encoder-decoder Transformer': 3,\n",
       "              'Causal decoder Transformer (decoder-only)': 7,\n",
       "              'Language modeling task (with Transformer architecture)': 10,\n",
       "              'Cloze task (with Transformer architecture)': 1,\n",
       "              'Dynamic batch size': 5,\n",
       "              'Adafactor optimizer': 1,\n",
       "              'Chain-of-thought': 2},\n",
       "             'Meta': {'Pre-normalization': 8,\n",
       "              'Learnable position embeddings': 4,\n",
       "              'Denoising autoencoding task (with Transformer architecture)': 1},\n",
       "             'Zhuiyi': {'Rotary position embeddings': 2},\n",
       "             'Baidu': {'Mixed precision training': 3},\n",
       "             'NVIDIA': {'Mixed precision training': 3}})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "innovation_occurrence_by_institution = defaultdict(dict)\n",
    "for innovation, institution_counts in institution_key_algorithm_occurrences.items():\n",
    "    for institution, count in institution_counts.items():\n",
    "        innovation_occurrence_by_institution[institution][innovation] = count\n",
    "innovation_occurrence_by_institution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "name": "Google (55 total)",
         "text": [
          "Transformer (general)",
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          null,
          "Language modeling task (with Transformer architecture)",
          null,
          null,
          null,
          null
         ],
         "textfont": {
          "color": "white",
          "size": 12
         },
         "type": "bar",
         "width": 1,
         "x": [
          "Transformer (general)",
          "Multi-Query Attention",
          "LayerNorm",
          "Sinusoidal position embeddings",
          "Relative position embeddings",
          "SwiGLU activation",
          "Sparsely-Gated Mixture-of-Experts layer (MoE)",
          "Encoder-decoder Transformer",
          "Causal decoder Transformer (decoder-only)",
          "Language modeling task (with Transformer architecture)",
          "Cloze task (with Transformer architecture)",
          "Dynamic batch size",
          "Adafactor optimizer",
          "Chain-of-thought"
         ],
         "xaxis": "x",
         "y": [
          10,
          2,
          7,
          1,
          3,
          2,
          1,
          3,
          7,
          10,
          1,
          5,
          1,
          2
         ],
         "yaxis": "y"
        },
        {
         "name": "OpenAI (19 total)",
         "text": [
          null,
          null,
          null,
          null,
          "Prompting for in-context learning"
         ],
         "textfont": {
          "color": "white",
          "size": 12
         },
         "type": "bar",
         "width": 1,
         "x": [
          "Kaplan et al. scaling laws",
          "Sparse Attention",
          "Instruction tuning",
          "RLHF",
          "Prompting for in-context learning"
         ],
         "xaxis": "x2",
         "y": [
          4,
          1,
          4,
          1,
          9
         ],
         "yaxis": "y2"
        },
        {
         "name": "Meta (13 total)",
         "text": [
          "Pre-normalization",
          null,
          null
         ],
         "textfont": {
          "color": "white",
          "size": 12
         },
         "type": "bar",
         "width": 1,
         "x": [
          "Pre-normalization",
          "Learnable position embeddings",
          "Denoising autoencoding task (with Transformer architecture)"
         ],
         "xaxis": "x3",
         "y": [
          8,
          4,
          1
         ],
         "yaxis": "y3"
        },
        {
         "name": "DeepMind (4 total)",
         "text": [
          null,
          null,
          null
         ],
         "textfont": {
          "color": "white",
          "size": 12
         },
         "type": "bar",
         "width": 1,
         "x": [
          "Hoffmann et al. scaling laws",
          "RLHF",
          "A2C"
         ],
         "xaxis": "x4",
         "y": [
          2,
          1,
          1
         ],
         "yaxis": "y4"
        },
        {
         "name": "Zhuiyi (2 total)",
         "text": [
          null
         ],
         "textfont": {
          "color": "white",
          "size": 12
         },
         "type": "bar",
         "width": 1,
         "x": [
          "Rotary position embeddings"
         ],
         "xaxis": "x5",
         "y": [
          2
         ],
         "yaxis": "y5"
        },
        {
         "name": "NVIDIA (3 total)",
         "text": [
          null
         ],
         "textfont": {
          "color": "white",
          "size": 12
         },
         "type": "bar",
         "width": 1,
         "x": [
          "Mixed precision training"
         ],
         "xaxis": "x6",
         "y": [
          3
         ],
         "yaxis": "y6"
        },
        {
         "name": "Baidu (3 total)",
         "text": [
          null
         ],
         "textfont": {
          "color": "white",
          "size": 12
         },
         "type": "bar",
         "width": 1,
         "x": [
          "Mixed precision training"
         ],
         "xaxis": "x7",
         "y": [
          3
         ],
         "yaxis": "y7"
        }
       ],
       "layout": {
        "autosize": false,
        "font": {
         "size": 10
        },
        "height": 250,
        "legend": {
         "orientation": "h",
         "x": 0,
         "y": 0
        },
        "margin": {
         "b": 0,
         "l": 0,
         "r": 0,
         "t": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "x": 0.5
        },
        "width": 480,
        "xaxis": {
         "anchor": "y",
         "categoryorder": "total descending",
         "domain": [
          0,
          0.5
         ],
         "showticklabels": false,
         "ticks": ""
        },
        "xaxis2": {
         "anchor": "y2",
         "categoryorder": "total descending",
         "domain": [
          0.5,
          0.6785714285714286
         ],
         "showticklabels": false,
         "ticks": ""
        },
        "xaxis3": {
         "anchor": "y3",
         "categoryorder": "total descending",
         "domain": [
          0.6785714285714286,
          0.7857142857142857
         ],
         "showticklabels": false,
         "ticks": ""
        },
        "xaxis4": {
         "anchor": "y4",
         "categoryorder": "total descending",
         "domain": [
          0.7857142857142857,
          0.8928571428571428
         ],
         "showticklabels": false,
         "ticks": ""
        },
        "xaxis5": {
         "anchor": "y5",
         "categoryorder": "total descending",
         "domain": [
          0.8928571428571428,
          0.9285714285714285
         ],
         "showticklabels": false,
         "ticks": ""
        },
        "xaxis6": {
         "anchor": "y6",
         "categoryorder": "total descending",
         "domain": [
          0.9285714285714285,
          0.9642857142857142
         ],
         "showticklabels": false,
         "ticks": ""
        },
        "xaxis7": {
         "anchor": "y7",
         "categoryorder": "total descending",
         "domain": [
          0.9642857142857142,
          0.9999999999999999
         ],
         "showticklabels": false,
         "ticks": ""
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Number of occurrences"
         }
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0,
          1
         ],
         "matches": "y",
         "showticklabels": false
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0,
          1
         ],
         "matches": "y",
         "showticklabels": false
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          1
         ],
         "matches": "y",
         "showticklabels": false
        },
        "yaxis5": {
         "anchor": "x5",
         "domain": [
          0,
          1
         ],
         "matches": "y",
         "showticklabels": false
        },
        "yaxis6": {
         "anchor": "x6",
         "domain": [
          0,
          1
         ],
         "matches": "y",
         "showticklabels": false
        },
        "yaxis7": {
         "anchor": "x7",
         "domain": [
          0,
          1
         ],
         "matches": "y",
         "showticklabels": false
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_institutions = len(innovation_occurrence_by_institution.keys())\n",
    "# Manual way to order the institutions\n",
    "institution_order = ['Google', 'OpenAI', 'Meta', 'DeepMind', 'Zhuiyi', 'NVIDIA', 'Baidu']\n",
    "\n",
    "# Create subplots with shared y-axis\n",
    "fig = make_subplots(\n",
    "    rows=1,\n",
    "    cols=num_institutions,\n",
    "    shared_yaxes=True,\n",
    "    column_widths=[0.5 * len(innovation_occurrence_by_institution[institution]) for institution in institution_order],\n",
    "    horizontal_spacing=0,\n",
    ")\n",
    "\n",
    "# Add traces\n",
    "for i, institution in enumerate(institution_order):\n",
    "    innovation_counts = innovation_occurrence_by_institution[institution]\n",
    "    x = list(innovation_counts.keys())\n",
    "    y = list(innovation_counts.values())\n",
    "    texts = [xi if yi > 7 else None for xi, yi in zip(x, y)]\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=x,\n",
    "            y=y,\n",
    "            name=f'{institution} ({sum(y)} total)',\n",
    "            text=texts,\n",
    "            textfont=dict(color='white', size=12),\n",
    "            # marker_color='#636EFA',\n",
    "            width=1\n",
    "        ), \n",
    "        row=1,\n",
    "        col=i+1\n",
    "    )\n",
    "    fig.update_xaxes(\n",
    "        categoryorder='total descending',\n",
    "        showticklabels=False,\n",
    "        ticks='',\n",
    "        col=i+1\n",
    "    )        \n",
    "\n",
    "## Plot layout\n",
    "fig.update_layout(\n",
    "    xaxis={'categoryorder':'total descending'},\n",
    "    # title='Occurrence of innovations in the top 10 largest LMs',\n",
    "    # xaxis_title='Company responsible for innovation',\n",
    "    yaxis_title='Number of occurrences',\n",
    "    legend=dict(\n",
    "        orientation='h',\n",
    "        y=0,\n",
    "        x=0,\n",
    "    ),\n",
    "    autosize=False,\n",
    "    width=480,\n",
    "    height=250,\n",
    "    # height=360,\n",
    "    title_x=0.5,\n",
    "    font=dict(size=10),\n",
    "    margin=dict(l=0, r=0, t=0, b=0),\n",
    ")\n",
    "\n",
    "save_plot(fig, result_file_location, 'key_innovations_occurrence_grouped')\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'Kaplan et al. scaling laws': {'OpenAI': 4},\n",
       "             'Hoffmann et al. scaling laws': {'DeepMind': 2},\n",
       "             'Transformer (general)': {'Google': 10},\n",
       "             'Sparse Attention': {'OpenAI': 1},\n",
       "             'Multi-Query Attention': {'Google': 2},\n",
       "             'LayerNorm': {'Google': 7},\n",
       "             'Pre-normalization': {'Meta': 8},\n",
       "             'Learnable position embeddings': {'Meta': 4},\n",
       "             'Sinusoidal position embeddings': {'Google': 1},\n",
       "             'Relative position embeddings': {'Google': 3},\n",
       "             'Rotary position embeddings': {'Zhuiyi': 2},\n",
       "             'SwiGLU activation': {'Google': 2},\n",
       "             'Sparsely-Gated Mixture-of-Experts layer (MoE)': {'Google': 1},\n",
       "             'Encoder-decoder Transformer': {'Google': 3},\n",
       "             'Causal decoder Transformer (decoder-only)': {'Google': 7},\n",
       "             'Language modeling task (with Transformer architecture)': {'Google': 10},\n",
       "             'Cloze task (with Transformer architecture)': {'Google': 1},\n",
       "             'Denoising autoencoding task (with Transformer architecture)': {'Meta': 1},\n",
       "             'Dynamic batch size': {'Google': 5},\n",
       "             'Adafactor optimizer': {'Google': 1},\n",
       "             'Mixed precision training': {'Baidu': 3, 'NVIDIA': 3},\n",
       "             'Instruction tuning': {'OpenAI': 4},\n",
       "             'RLHF': {'OpenAI': 1, 'DeepMind': 1},\n",
       "             'A2C': {'DeepMind': 1},\n",
       "             'Prompting for in-context learning': {'OpenAI': 9},\n",
       "             'Chain-of-thought': {'Google': 2}})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "institution_key_algorithm_occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names = set()\n",
    "for innovation, company_counts in institution_key_algorithm_occurrences.items():\n",
    "    company_names.update(list(company_counts.keys()))\n",
    "company_names = list(company_names)\n",
    "labels = company_names\n",
    "parents = [''] * len(company_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_values = defaultdict(list)\n",
    "for innovation, company_counts in institution_key_algorithm_occurrences.items():\n",
    "    for company_name, count in company_counts.items():\n",
    "        company_values[company_name].append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [sum(company_values[company_name]) for company_name in company_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Meta', 'Baidu', 'NVIDIA', 'Zhuiyi', 'Google', 'DeepMind', 'OpenAI']\n",
      "['', '', '', '', '', '', '']\n",
      "[13, 3, 3, 2, 55, 4, 19]\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(parents)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for innovation, company_counts in institution_key_algorithm_occurrences.items():\n",
    "    for company_name, count in company_counts.items():\n",
    "        labels.append(innovation)\n",
    "        parents.append(company_name)\n",
    "        values.append(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Meta', 'Baidu', 'NVIDIA', 'Zhuiyi', 'Google', 'DeepMind', 'OpenAI', 'Kaplan et al. scaling laws', 'Hoffmann et al. scaling laws', 'Transformer (general)', 'Sparse Attention', 'Multi-Query Attention', 'LayerNorm', 'Pre-normalization', 'Learnable position embeddings', 'Sinusoidal position embeddings', 'Relative position embeddings', 'Rotary position embeddings', 'SwiGLU activation', 'Sparsely-Gated Mixture-of-Experts layer (MoE)', 'Encoder-decoder Transformer', 'Causal decoder Transformer (decoder-only)', 'Language modeling task (with Transformer architecture)', 'Cloze task (with Transformer architecture)', 'Denoising autoencoding task (with Transformer architecture)', 'Dynamic batch size', 'Adafactor optimizer', 'Mixed precision training', 'Mixed precision training', 'Instruction tuning', 'RLHF', 'RLHF', 'A2C', 'Prompting for in-context learning', 'Chain-of-thought']\n",
      "['', '', '', '', '', '', '', 'OpenAI', 'DeepMind', 'Google', 'OpenAI', 'Google', 'Google', 'Meta', 'Meta', 'Google', 'Google', 'Zhuiyi', 'Google', 'Google', 'Google', 'Google', 'Google', 'Google', 'Meta', 'Google', 'Google', 'Baidu', 'NVIDIA', 'OpenAI', 'OpenAI', 'DeepMind', 'DeepMind', 'OpenAI', 'Google']\n",
      "[13, 3, 3, 2, 55, 4, 19, 4, 2, 10, 1, 2, 7, 8, 4, 1, 3, 2, 2, 1, 3, 7, 10, 1, 1, 5, 1, 3, 3, 4, 1, 1, 1, 9, 2]\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(parents)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "labels": [
          "Meta",
          "Baidu",
          "NVIDIA",
          "Zhuiyi",
          "Google",
          "DeepMind",
          "OpenAI",
          "Kaplan et al. scaling laws",
          "Hoffmann et al. scaling laws",
          "Transformer (general)",
          "Sparse Attention",
          "Multi-Query Attention",
          "LayerNorm",
          "Pre-normalization",
          "Learnable position embeddings",
          "Sinusoidal position embeddings",
          "Relative position embeddings",
          "Rotary position embeddings",
          "SwiGLU activation",
          "Sparsely-Gated Mixture-of-Experts layer (MoE)",
          "Encoder-decoder Transformer",
          "Causal decoder Transformer (decoder-only)",
          "Language modeling task (with Transformer architecture)",
          "Cloze task (with Transformer architecture)",
          "Denoising autoencoding task (with Transformer architecture)",
          "Dynamic batch size",
          "Adafactor optimizer",
          "Mixed precision training",
          "Mixed precision training",
          "Instruction tuning",
          "RLHF",
          "RLHF",
          "A2C",
          "Prompting for in-context learning",
          "Chain-of-thought"
         ],
         "parents": [
          "",
          "",
          "",
          "",
          "",
          "",
          "",
          "OpenAI",
          "DeepMind",
          "Google",
          "OpenAI",
          "Google",
          "Google",
          "Meta",
          "Meta",
          "Google",
          "Google",
          "Zhuiyi",
          "Google",
          "Google",
          "Google",
          "Google",
          "Google",
          "Google",
          "Meta",
          "Google",
          "Google",
          "Baidu",
          "NVIDIA",
          "OpenAI",
          "OpenAI",
          "DeepMind",
          "DeepMind",
          "OpenAI",
          "Google"
         ],
         "textinfo": "label+value",
         "type": "treemap",
         "values": [
          13,
          3,
          3,
          2,
          55,
          4,
          19,
          4,
          2,
          10,
          1,
          2,
          7,
          8,
          4,
          1,
          3,
          2,
          2,
          1,
          3,
          7,
          10,
          1,
          1,
          5,
          1,
          3,
          3,
          4,
          1,
          1,
          1,
          9,
          2
         ]
        }
       ],
       "layout": {
        "font": {
         "size": 12
        },
        "height": 360,
        "margin": {
         "b": 20,
         "l": 20,
         "r": 20,
         "t": 30
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "width": 480
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure(go.Treemap(\n",
    "    labels=labels,\n",
    "    parents=parents,\n",
    "    values=values,\n",
    "    textinfo=\"label+value\",\n",
    "))\n",
    "\n",
    "## Plot layout\n",
    "fig.update_layout(\n",
    "    width=480,\n",
    "    height=360,\n",
    "    font=dict(size=12),\n",
    "    # uniformtext=dict(minsize=6, mode='hide'),\n",
    "    margin=dict(l=20, r=20, t=30, b=20),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "labels": [
          "Google",
          "OpenAI",
          "Meta",
          "Algorithm A",
          "Algorithm B",
          "Algorithm C",
          "Algorithm D",
          "Algorithm E",
          "Algorithm F"
         ],
         "marker": {
          "colors": [
           "blue",
           "purple",
           "green",
           "lightblue",
           "lightblue",
           "lightblue",
           "lightpurple",
           "lightpurple",
           "lightgreen"
          ]
         },
         "parents": [
          "",
          "",
          "",
          "Google",
          "Google",
          "Google",
          "OpenAI",
          "OpenAI",
          "Meta"
         ],
         "textinfo": "label+value",
         "type": "treemap",
         "values": [
          40,
          20,
          20,
          10,
          10,
          20,
          10,
          10,
          20
         ]
        }
       ],
       "layout": {
        "font": {
         "size": 10
        },
        "height": 360,
        "margin": {
         "b": 20,
         "l": 20,
         "r": 20,
         "t": 30
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "white",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "#C8D4E3",
             "linecolor": "#C8D4E3",
             "minorgridcolor": "#C8D4E3",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "white",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "#C8D4E3"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "white",
          "polar": {
           "angularaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           },
           "bgcolor": "white",
           "radialaxis": {
            "gridcolor": "#EBF0F8",
            "linecolor": "#EBF0F8",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "yaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           },
           "zaxis": {
            "backgroundcolor": "white",
            "gridcolor": "#DFE8F3",
            "gridwidth": 2,
            "linecolor": "#EBF0F8",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#EBF0F8"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           },
           "bgcolor": "white",
           "caxis": {
            "gridcolor": "#DFE8F3",
            "linecolor": "#A2B1C6",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "#EBF0F8",
           "linecolor": "#EBF0F8",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#EBF0F8",
           "zerolinewidth": 2
          }
         }
        },
        "width": 480
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Sample Data\n",
    "labels = [\"Google\", \"OpenAI\", \"Meta\", \n",
    "          \"Algorithm A\", \"Algorithm B\", \"Algorithm C\", \n",
    "          \"Algorithm D\", \"Algorithm E\",\n",
    "          \"Algorithm F\"]\n",
    "\n",
    "parents = [\"\", \"\", \"\", \n",
    "           \"Google\", \"Google\", \"Google\", \n",
    "           \"OpenAI\", \"OpenAI\",\n",
    "           \"Meta\"]\n",
    "\n",
    "values = [40, 20, 20,   # Company total values\n",
    "          10, 10, 20,   # Google's algorithm values\n",
    "          10, 10,       # OpenAI's algorithm values\n",
    "          20]           # Meta's algorithm value\n",
    "\n",
    "fig = go.Figure(go.Treemap(\n",
    "    labels=labels,\n",
    "    parents=parents,\n",
    "    values=values,\n",
    "    marker_colors=[\"blue\", \"purple\", \"green\"] + [\"lightblue\"]*3 + [\"lightpurple\"]*2 + [\"lightgreen\"],\n",
    "    textinfo=\"label+value\"\n",
    "))\n",
    "\n",
    "## Plot layout\n",
    "fig.update_layout(\n",
    "    width=480,\n",
    "    # height=250,\n",
    "    height=360,\n",
    "    font=dict(size=10),\n",
    "    margin=dict(l=20, r=20, t=30, b=20),\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "epoch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
